{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a prediction model for the final selling price of the auctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT REMARK:\n",
    "\n",
    "This code shall be executed from start to finish in the defined order. Errors may occur if the cells are executed in a different order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "    \n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, different prediction models to predict the final selling price of an auction before it starts have been built. The predictions are made with the intention that having an estimation of the final selling price of an auction could be very helpful for auction business owners (in this case Swoopo).\n",
    "\n",
    "Since the final selling price of an auction item is obtained by multiplying the bid price increment for that auction and the number of bids placed, and the bid price increment for an auction is known data, we are also indirectly predicting the number of bids placed for the auction. Swoopo's gain is obtained by multiplying the number of bids placed (predicted) and the bid fee (known data), and summing the result to the final price of the auction (predicted). Therefore, Swoopo's gain is also being indirectly predicted. Swoopo's profit would be calculated as the difference between the gain (predicted) and the retail price of the item (known data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auction_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>desc</th>\n",
       "      <th>retail</th>\n",
       "      <th>price</th>\n",
       "      <th>finalprice</th>\n",
       "      <th>bidincrement</th>\n",
       "      <th>bidfee</th>\n",
       "      <th>winner</th>\n",
       "      <th>...</th>\n",
       "      <th>freebids</th>\n",
       "      <th>endtime_str</th>\n",
       "      <th>flg_click_only</th>\n",
       "      <th>flg_beginnerauction</th>\n",
       "      <th>flg_fixedprice</th>\n",
       "      <th>flg_endprice</th>\n",
       "      <th>bids_placed</th>\n",
       "      <th>swoopo_sale_price</th>\n",
       "      <th>swoopo_profit</th>\n",
       "      <th>winner_benefit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86827</td>\n",
       "      <td>10009602</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>13.35</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Racer11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-16 19:52:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>77.060489</td>\n",
       "      <td>-422.929511</td>\n",
       "      <td>467.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   auction_id  product_id                                      item  \\\n",
       "0       86827    10009602  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "\n",
       "                                            desc  retail  price  finalprice  \\\n",
       "0  Sony Ericsson S500i Unlocked Mysterious Green  499.99  13.35       13.35   \n",
       "\n",
       "   bidincrement  bidfee   winner       ...        freebids  \\\n",
       "0          0.15    0.75  Racer11       ...               0   \n",
       "\n",
       "           endtime_str flg_click_only  flg_beginnerauction  flg_fixedprice  \\\n",
       "0  2008-09-16 19:52:00              0                    0               0   \n",
       "\n",
       "   flg_endprice  bids_placed  swoopo_sale_price  swoopo_profit  winner_benefit  \n",
       "0             0         89.0          77.060489    -422.929511          467.14  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomesDf = pd.read_csv('./outcomes_clean.tsv',sep='\\t')\n",
    "outcomesDf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that are given as input for the prediction model are: \"retail\" (the retail price of the item), \"bidincrement\" (the bid increment), \"bidfee\" (the bid fee), and the flags that indicate whether the auction is a click-only auction, a beginner auction, a fixed-price auction or and end-price auction. \n",
    "\n",
    "The retail price of the item will clearly influence the final price that the auction reaches (in general, users will place more bids for more expensive items). The bid increment directly influences the final price that the auction reaches, because the last one is calculated by multiplying the bid increment and the number of bids placed in the auction. The bid fee will also influence the final price of the item, since users will be willing to place more bids if the bid fee is low, and placing more bids implies that the final price reached increases. The flags indicating the type of auction might have an influence for some users on whether they are willing to place more or less bids for the auction, so that is why they have been included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_column_names = [\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"auction_id\" (auction id) column has not been included as an input variable for the model because it is unique for each auction, and therefore, it does not provide any useful information for the predictions.\n",
    "\n",
    "The columns \"product_id\" (product id), \"desc\" (product description) and \"item\" have not been included as input for the model either. The product that is being sold (i.e., the product id) can certainly influence the final price that the auction reaches, but if we were to include this variable as input for the model, we would have to one-hot encode it, as it is expressed as a number, but there is no actual order defined for the values that it contains (a product with a larger product id than another one is not \"larger\" than that product. The number is just used as an identifier). If we were to include the other two columns (\"desc\", containing the product description, and \"item\", containing an item string associated to the product), they would also have to be one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, all of these columns contain many unique values. In the previous section, the item categories have been obtained from Amazon, and have been converted into a Word2Vec vector. The distances that these vectors define can be used to create clusters for the product categories that can be used as input for the prediction models rather than the other mentioned columns (which contain way too many different values, and the prediction model would not be able to generalize well if we used them as input variables to express diferences between the type of auctioned items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080\n",
      "1769\n",
      "1767\n"
     ]
    }
   ],
   "source": [
    "print(len(outcomesDf[\"product_id\"].unique()))\n",
    "print(len(outcomesDf[\"desc\"].unique()))\n",
    "print(len(outcomesDf[\"item\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"price\" and \"finalprice\" contain the final selling price reached for the auction (with a small difference between each other that will later be reminded), which is exactly what we want to predict, and therefore, they cannot be an input for the prediction model.\n",
    "\n",
    "The columns \"winner\" (winner of the auction) 'placedbids' (number of paid bids placed by the winnner of the auction), \"freebids\" (number of free bids placed by the winner of the auction) and \"endtime_str' (time in which the auction finished) contain information that is not known until the auction finishes, and therefore, they cannot be used as input for the model to predict the final selling price before the auction starts.\n",
    "\n",
    "The same thing happens with the other columns that were added in previous sections to the dataset: \"bids_placed\" (total number of bids placed for the auction), \"swoopo_sale_price\" (total gain obtained by Swoopo for the auction), \"swoopo_profit\" (total profit obtained by Swoopo for the auction, calculated as the gain obtained minus the retail price of the item), and \"winner_benefit\" (the difference between the money paid by the winner for the item and the retail price of the item). They cannot be used as input data for the model because they contain data that is not known at the beginning of the auction (and also, even if the data was known at the beginning of the auction, they have been calculated using the values of the other variables, so they could lead to multicollinearity problems if they were included in that case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns \"flg_fixedprice\" and \"flg_endprice\" indicate whether the auction is a fixed-price auction or and end-price auction. Fixed-price actions are auctions for which the final selling price of the item is set from the beginning. End-price auctions are auctions in which the final selling price of the item is zero independently of the bids that are placed, and the revenue for Swoopo comes exclusively from the bids that are placed).\n",
    "\n",
    "The column \"finalprice\" contains the real final selling price of the auction (that is, the fixed price for fixed-price auctions and zero for end-price auctions), while the \"price\" column contains the selling price of the auction calculated as the number of bids placed multiplied by the bid increment. Since the final selling price is already known from the beginning for fixed-price auctions and end-price auctions, and we are interested in knowing the gain that the auction provides for Swoopo, it has been chosen to predict the value in column \"price\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = outcomesDf[x_column_names]\n",
    "y = outcomesDf[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Single model - Without having into account the product categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first models that have been built do not take into account the product categories. The input variables for the model are: the retail price of the item, the bid increment, the bid fee, and the flags that indicate whether the auction is a click-only auction, a beginner auction, a fixed-price auction or and end-price auction.\n",
    "\n",
    "In order to be able to compare the results for different models, it has been chosen to develop the following ones: a random forest regressor, a k-neighbors regressor, a decision tree regressor, a linear regression and a RANSAC regressor.\n",
    "\n",
    "The dataset has been divided into different parts for the training and the testing phases, and 5-fold cross-validation has been applied so that the resulting quality metrics are more reliable.\n",
    "\n",
    "The metrics that have been calculated are the mean absolute error and the median absolute error. \n",
    "\n",
    "The mean absolute error gives an idea about the error that the model incurs in when performing final price predictions for individual auctions. Nevertheless, the final selling price of an auction depends on a lot of things: the users that are monitoring the auction, the amount of money that they have and their interests at the moment of the auction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when the item that is auctioned is the same one, the final price that is reached for each one of the auctions can be very different. As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>finalprice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>19.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>47.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>55.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>86.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>48.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>113.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>75.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>35.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>19.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>36.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>41.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>123.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>49.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>63.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             desc  finalprice\n",
       "0   Sony Ericsson S500i Unlocked Mysterious Green       13.35\n",
       "3   Sony Ericsson S500i Unlocked Mysterious Green       19.65\n",
       "4   Sony Ericsson S500i Unlocked Mysterious Green       47.10\n",
       "5   Sony Ericsson S500i Unlocked Mysterious Green       55.20\n",
       "6   Sony Ericsson S500i Unlocked Mysterious Green       86.10\n",
       "17  Sony Ericsson S500i Unlocked Mysterious Green       48.60\n",
       "18  Sony Ericsson S500i Unlocked Mysterious Green      113.10\n",
       "48  Sony Ericsson S500i Unlocked Mysterious Green       75.15\n",
       "49  Sony Ericsson S500i Unlocked Mysterious Green       35.10\n",
       "50  Sony Ericsson S500i Unlocked Mysterious Green       19.20\n",
       "53  Sony Ericsson S500i Unlocked Mysterious Green       36.60\n",
       "54  Sony Ericsson S500i Unlocked Mysterious Green       41.55\n",
       "55  Sony Ericsson S500i Unlocked Mysterious Green      123.75\n",
       "56  Sony Ericsson S500i Unlocked Mysterious Green       49.50\n",
       "57  Sony Ericsson S500i Unlocked Mysterious Green       63.75"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomesDf[outcomesDf[\"desc\"] == \"Sony Ericsson S500i Unlocked Mysterious Green\"][[\"desc\",\"finalprice\"]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most extreme values could be considered outliers and be removed from the dataset so that they do not negatively influence the model. However, in this case, the extreme values are legitimate observations and they are most likely not incorrectly entered data. Therefore, instead of removing them, it has been decided to include some prediction models that are robust to outliers. For example, random forests and decision trees isolate atypical observations into small leaves, and the RANSAC algorithm provides a robust linear model estimation.\n",
    "\n",
    "Because of the high variance of the final selling price of the auctions, appart from the mean absolute error, it has also been decided to use the median absolute error as a metric. This metric is not be so highly influenced by extreme values. Making final selling price predictions can help Swoopo or similar businesses to have an estimation of the money that they will earn before a group of auctions start. However, we consider it more important to have an accurate estimation of groups of several auctions (for example, accurate weekly or monthly profit estimations, which can be calculated by using the estimations of the final price of the auctions included in those time periods), rather than accurate estimations for individual auctions, since Swoopo will profit of the total result of all of their auctions. Because of this reason, although the mean absolute error has been calculated, the median absolute error is considered to be a more appropiate metric in this case.\n",
    "\n",
    "The mean absolute percentage error (MAPE) has also been considered as a metric. Nevertheless, if a video game is predicted to reach a selling price of 20\\$, and the real selling price ends up being 30\\$,the MAPE would be 33%. However, if a laptop is predicted to reach a selling price of 700\\$ and the real selling price ends up being 800\\$, the MAPE is only 12.50%, but there is a prediction error of 100\\$ between the predicted price and the final price, which can have a higher impact in the business than the error in the video game prediction. For this reason, it has been decided not to use a metric that indicates a percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, in this section, the models that have been built do not take into account the product categories. The input variables for the models are: the retail price of the item, the bid increment, the bid fee, and the flags that indicate whether the auction is a click-only auction, a beginner auction, a fixed-price auction or and end-price auction.\n",
    "\n",
    "The following code is used to calculate the mean absolute error and the median absolute error (as the average of the results obtained for these two metrics over the 5 cross-validation folds) for the previously mentioned models. The results are analyzed after the lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "--\n",
      "Random Forest Regressor\n",
      "Median absolute error: 11.1752458141\n",
      "Mean absolute error: 29.3445832716\n",
      "--\n",
      "K Neighbors Regressor\n",
      "Median absolute error: 11.9772\n",
      "Mean absolute error: 31.8406889199\n",
      "--\n",
      "Decision Tree Regressor\n",
      "Median absolute error: 11.0488437347\n",
      "Mean absolute error: 29.7480416922\n",
      "--\n",
      "Linear Regression\n",
      "Median absolute error: 24.2493912238\n",
      "Mean absolute error: 42.0046659273\n",
      "--\n",
      "RANSAC Regressor\n",
      "Median absolute error: 10.4691486861\n",
      "Mean absolute error: 38.1582942494\n"
     ]
    }
   ],
   "source": [
    "kFoldMedianAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMedianAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMedianAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMedianAbsoluteErrorsLinearRegression = []\n",
    "kFoldMedianAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kFoldMeanAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMeanAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMeanAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMeanAbsoluteErrorsLinearRegression = []\n",
    "kFoldMeanAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kFoldNumber = 1\n",
    "kf = KFold(n_splits=5, random_state=1)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #5-fold cross-validation\n",
    "    print(\"Executing k fold = \"+str(kFoldNumber))\n",
    "    kFoldNumber=kFoldNumber+1\n",
    "    \n",
    "    #train and test split\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    #RandomForestRegressor\n",
    "    model=RandomForestRegressor(random_state=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRandomForestRegressor.append(kFoldMedianAbsoluteError)    \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRandomForestRegressor.append(kFoldMeanAbsoluteError)  \n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    model = KNeighborsRegressor()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsKNeighborsRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsKNeighborsRegressor.append(kFoldMeanAbsoluteError)\n",
    "    \n",
    "    #DecisionTreeRegressor\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsDecisionTreeRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsDecisionTreeRegressor.append(kFoldMeanAbsoluteError)\n",
    "    \n",
    "    #LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsLinearRegression.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsLinearRegression.append(kFoldMeanAbsoluteError)\n",
    "    \n",
    "    #RANSACRegressor\n",
    "    model = RANSACRegressor(random_state=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRANSACRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRANSACRegressor.append(kFoldMeanAbsoluteError)\n",
    "    \n",
    "medianAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRandomForestRegressor)\n",
    "meanAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRandomForestRegressor)\n",
    "medianAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsKNeighborsRegressor)\n",
    "meanAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsKNeighborsRegressor)\n",
    "medianAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsDecisionTreeRegressor)\n",
    "meanAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsDecisionTreeRegressor)\n",
    "medianAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMedianAbsoluteErrorsLinearRegression)\n",
    "meanAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMeanAbsoluteErrorsLinearRegression)\n",
    "medianAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRANSACRegressor)\n",
    "meanAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRANSACRegressor)\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Random Forest Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"K Neighbors Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Decision Tree Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Linear Regression\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"--\")\n",
    "print(\"RANSAC Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRANSACRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRANSACRegressorAverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the model with the best results in terms of the median absolute error is the RANSAC regressor. Nevertheless, its mean absolute error is much higher than for other models, which indicates that the deviation between individual prediction results and real results is higher than for other models. The decision tree regresor and the random forest regressor both present good results for the two metrics. The k-neighbors regressor performs alright, but worse than the other two. The worst results are obtained with the linear regression, probably because it is more sensitive to outliers than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Single Model - Clustering by product categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second set of models that have into account the product categories have been built. As specified in previous sections, with the use of the product description available for each product in the column \"desc\" of the dataset, the Amazon product category has been obtained. A word embedding vector representing the product category has been calculated based on Google's pre-trained Word2Vec model. The distances defined by these vectors can be used to perform a distance-based clustering. For these models, a K-Means clustering has been used to group the products belonging to semantically similar categories.\n",
    "\n",
    "The idea behind categorizing the products is that, if the final selling price of the items have similar patterns for similar products, a prediction model will perform better if a grouping based on the product categories is given as input. In this case, the input is the cluster identifier that each product belongs to. Since this is a categorical integer feature, a one-hot encoder has been used.\n",
    "\n",
    "Therefore, for this set of models, the input variables are: the retail price of the item, the bid increment, the bid fee, the flags that indicate the type of auction, and the one-hot encoded cluster identifier. The prediction models and the metrics are the same as in the previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to take into account for these set of models is that they require a preprocessing step that has to be performed both for the training data and the test data. This preprocessing step consists on, given the product description of an item in the column \"desc\", its Amazon category must be obtained, and afterwards, the corresponding word embedding vector.\n",
    "\n",
    "Also, the product category clustering must be performed with the training data. Each product appearing in the test data is then assigned one of the clusters obtained during the training part. The cluster that it is assigned to is the nearest one (according to the distance defined by the word embedding vector of the product appearing in the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomesDf = pd.read_csv('./outcomes_clean.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file contains the word embedding vectors associated to each product in the dataset that have been calculated in previous sections. If the prediction models were to be used in real life, a word embedding vector would have to be calculated for each new product following the procedure explained in previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDescriptionToVectorDf = pd.read_excel('./productDescriptionToVector.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is given as input a DataFrame that contains the cluster assigned to each product. The output of the function is the one-hot encoded version of the DataFrame given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCategoryDataFrameToOneHotEncodedVersion(clusteredCategoriesDf, categoryColumnName, trainClusterColumNames = None):\n",
    "    #categories assigned to each row of the DataFrame\n",
    "    categories = clusteredCategoriesDf[categoryColumnName]\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    categories = categories.values.reshape(len(categories), 1)\n",
    "    #One-Hot encoded version of the categories, where each column represents a category,\n",
    "    #and each row has the value \"1\" on the column of the category that it belongs to,\n",
    "    #and \"0\" on the other columns.\n",
    "    onehot_encoded = onehot_encoder.fit_transform(categories)\n",
    "    ncolumns = np.shape(onehot_encoded)[1]\n",
    "    #array that will contain the column names for the category clusters\n",
    "    clusterColumNames = ['cluster_']*ncolumns\n",
    "    for clusterNumber in np.arange(ncolumns):\n",
    "        #each column name is \"cluster_\" followed by the cluster number\n",
    "        clusterColumNames[clusterNumber] += str(clusterNumber)\n",
    "\n",
    "    #One-Hot Encoded DataFrame\n",
    "    clusteredCategoriesDfOneHotEnc = pd.DataFrame(onehot_encoded,columns=clusterColumNames)\n",
    "    \n",
    "    if trainClusterColumNames is not None:\n",
    "        #this function is called both in the training phase and the testing phase.\n",
    "        #For the training phase, trainClusterColumNames = None\n",
    "        #For the testing phase, trainClusterColumNames contains the column names\n",
    "        #of the One-Hot encoded DataFrame obtained during the training phase.\n",
    "        for trainColumnName in trainClusterColumNames:\n",
    "            if trainColumnName not in clusterColumNames:\n",
    "                #It may happen that a product category that appeared in the training phase\n",
    "                #does not appear in the testing phase. However, since the model is given \n",
    "                #the training One-Hot encoded DataFrame column values as input, it is necessary\n",
    "                #that the testing One-Hot encoded DataFrame contains the same columns\n",
    "                \n",
    "                #Therefore, if a category that appeared in the training phase does not\n",
    "                #appear in the testing phase, a column filled in with \"0\" for all rows\n",
    "                #is created.\n",
    "                clusteredCategoriesDfOneHotEnc[trainColumnName] = 0.0\n",
    "            \n",
    "    return clusteredCategoriesDfOneHotEnc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as in the previous set of models, the following lines of code calculate the metrics associated to each one of the prediction models using 5-fold cross-validation.\n",
    "\n",
    "During the training part, the K-Means algorithm is used to obtain clusters for the word embedding vectors. \n",
    "\n",
    "The dataset contains several auctions in which the same item was auctioned. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auction_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>desc</th>\n",
       "      <th>retail</th>\n",
       "      <th>price</th>\n",
       "      <th>finalprice</th>\n",
       "      <th>bidincrement</th>\n",
       "      <th>bidfee</th>\n",
       "      <th>winner</th>\n",
       "      <th>...</th>\n",
       "      <th>freebids</th>\n",
       "      <th>endtime_str</th>\n",
       "      <th>flg_click_only</th>\n",
       "      <th>flg_beginnerauction</th>\n",
       "      <th>flg_fixedprice</th>\n",
       "      <th>flg_endprice</th>\n",
       "      <th>bids_placed</th>\n",
       "      <th>swoopo_sale_price</th>\n",
       "      <th>swoopo_profit</th>\n",
       "      <th>winner_benefit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86827</td>\n",
       "      <td>10009602</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>13.35</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Racer11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-16 19:52:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>77.060489</td>\n",
       "      <td>-422.929511</td>\n",
       "      <td>467.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88638</td>\n",
       "      <td>10006115</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>19.65</td>\n",
       "      <td>19.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Mokkis</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-08-23 22:02:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>113.426113</td>\n",
       "      <td>-386.563887</td>\n",
       "      <td>472.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88639</td>\n",
       "      <td>10006115</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>47.10</td>\n",
       "      <td>47.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Superloeffel</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-08-24 14:23:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>271.876331</td>\n",
       "      <td>-228.113669</td>\n",
       "      <td>392.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>88693</td>\n",
       "      <td>10008975</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>55.20</td>\n",
       "      <td>55.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Danydemir80</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>2008-08-22 22:44:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>318.632133</td>\n",
       "      <td>-181.357867</td>\n",
       "      <td>444.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>88694</td>\n",
       "      <td>10008975</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>86.10</td>\n",
       "      <td>86.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Destination8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-08-24 07:10:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>496.996860</td>\n",
       "      <td>-2.993140</td>\n",
       "      <td>371.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>90526</td>\n",
       "      <td>10009642</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>48.60</td>\n",
       "      <td>48.60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Wadenbeisser</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-02 00:06:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>280.534813</td>\n",
       "      <td>-219.455187</td>\n",
       "      <td>431.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>90527</td>\n",
       "      <td>10009642</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>113.10</td>\n",
       "      <td>113.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Vonluxburg</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-04 02:27:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>754.0</td>\n",
       "      <td>652.849533</td>\n",
       "      <td>152.859533</td>\n",
       "      <td>172.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>92708</td>\n",
       "      <td>10009642</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>75.15</td>\n",
       "      <td>75.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>pauli55</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-08-25 15:12:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>433.789942</td>\n",
       "      <td>-66.200058</td>\n",
       "      <td>290.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>92709</td>\n",
       "      <td>10009642</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>35.10</td>\n",
       "      <td>35.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Voovoo3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-05 22:33:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>202.608476</td>\n",
       "      <td>-297.381524</td>\n",
       "      <td>459.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>92723</td>\n",
       "      <td>10009602</td>\n",
       "      <td>sony-ericsson-s500i-unlocked-mysterious-</td>\n",
       "      <td>Sony Ericsson S500i Unlocked Mysterious Green</td>\n",
       "      <td>499.99</td>\n",
       "      <td>19.20</td>\n",
       "      <td>19.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.75</td>\n",
       "      <td>barrakuda</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-09-20 18:35:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>110.828568</td>\n",
       "      <td>-389.161432</td>\n",
       "      <td>444.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    auction_id  product_id                                      item  \\\n",
       "0        86827    10009602  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "3        88638    10006115  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "4        88639    10006115  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "5        88693    10008975  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "6        88694    10008975  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "17       90526    10009642  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "18       90527    10009642  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "48       92708    10009642  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "49       92709    10009642  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "50       92723    10009602  sony-ericsson-s500i-unlocked-mysterious-   \n",
       "\n",
       "                                             desc  retail   price  finalprice  \\\n",
       "0   Sony Ericsson S500i Unlocked Mysterious Green  499.99   13.35       13.35   \n",
       "3   Sony Ericsson S500i Unlocked Mysterious Green  499.99   19.65       19.65   \n",
       "4   Sony Ericsson S500i Unlocked Mysterious Green  499.99   47.10       47.10   \n",
       "5   Sony Ericsson S500i Unlocked Mysterious Green  499.99   55.20       55.20   \n",
       "6   Sony Ericsson S500i Unlocked Mysterious Green  499.99   86.10       86.10   \n",
       "17  Sony Ericsson S500i Unlocked Mysterious Green  499.99   48.60       48.60   \n",
       "18  Sony Ericsson S500i Unlocked Mysterious Green  499.99  113.10      113.10   \n",
       "48  Sony Ericsson S500i Unlocked Mysterious Green  499.99   75.15       75.15   \n",
       "49  Sony Ericsson S500i Unlocked Mysterious Green  499.99   35.10       35.10   \n",
       "50  Sony Ericsson S500i Unlocked Mysterious Green  499.99   19.20       19.20   \n",
       "\n",
       "    bidincrement  bidfee        winner       ...        freebids  \\\n",
       "0           0.15    0.75       Racer11       ...               0   \n",
       "3           0.15    0.75        Mokkis       ...               0   \n",
       "4           0.15    0.75  Superloeffel       ...               0   \n",
       "5           0.15    0.75   Danydemir80       ...              13   \n",
       "6           0.15    0.75  Destination8       ...               0   \n",
       "17          0.15    0.75  Wadenbeisser       ...               0   \n",
       "18          0.15    0.75    Vonluxburg       ...               0   \n",
       "48          0.15    0.75       pauli55       ...               0   \n",
       "49          0.15    0.75       Voovoo3       ...               0   \n",
       "50          0.15    0.75     barrakuda       ...               0   \n",
       "\n",
       "            endtime_str flg_click_only  flg_beginnerauction  flg_fixedprice  \\\n",
       "0   2008-09-16 19:52:00              0                    0               0   \n",
       "3   2008-08-23 22:02:00              0                    0               0   \n",
       "4   2008-08-24 14:23:00              0                    0               0   \n",
       "5   2008-08-22 22:44:00              0                    0               0   \n",
       "6   2008-08-24 07:10:00              0                    0               0   \n",
       "17  2008-09-02 00:06:00              0                    0               0   \n",
       "18  2008-09-04 02:27:00              0                    0               0   \n",
       "48  2008-08-25 15:12:00              0                    0               0   \n",
       "49  2008-09-05 22:33:00              0                    0               0   \n",
       "50  2008-09-20 18:35:00              0                    0               0   \n",
       "\n",
       "    flg_endprice  bids_placed  swoopo_sale_price  swoopo_profit  \\\n",
       "0              0         89.0          77.060489    -422.929511   \n",
       "3              0        131.0         113.426113    -386.563887   \n",
       "4              0        314.0         271.876331    -228.113669   \n",
       "5              0        368.0         318.632133    -181.357867   \n",
       "6              0        574.0         496.996860      -2.993140   \n",
       "17             0        324.0         280.534813    -219.455187   \n",
       "18             0        754.0         652.849533     152.859533   \n",
       "48             0        501.0         433.789942     -66.200058   \n",
       "49             0        234.0         202.608476    -297.381524   \n",
       "50             0        128.0         110.828568    -389.161432   \n",
       "\n",
       "    winner_benefit  \n",
       "0           467.14  \n",
       "3           472.84  \n",
       "4           392.89  \n",
       "5           444.79  \n",
       "6           371.14  \n",
       "17          431.89  \n",
       "18          172.39  \n",
       "48          290.59  \n",
       "49          459.64  \n",
       "50          444.04  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomesDf[outcomesDf[\"desc\"] == \"Sony Ericsson S500i Unlocked Mysterious Green\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the item that is auctioned is the same in these cases, the description of the item (and so, its word embedding vector) is the same. In each iteration, the K-Means algorithm assigns every data point to the nearest centroid. Replication of data points (the word embedding vectors in this case) influences the centroids of the clusters.\n",
    "\n",
    "The objective of this clustering is to provide the prediction model with a grouping of the items according to their categories, so that it is easier for the model to find a pattern (if it exists) within products belonging to the same category. Duplicated items have been removed from the input data given to the clustering algorithm so that each different product has the same weight for the product category clustering.\n",
    "\n",
    "The duplicated items have instead not been removed from the input data given to the prediction algorithms, so that the weight of the evidence is higher on cases of duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that each different item has been assigned a cluster, a DataFrame containing the product description and the corresponding cluster number is created. This DataFrame is given as input for the function explained above, which returns the one-hot encoded version of the DataFrame. The input variables for the prediction models are the retail price of the item, the bid increment, the bid fee, the flags that indicate the type of auction, and the one-hot encoded cluster identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each row contained in the test data, the word embedding vector associated to the product description is obtained. Then, the nearest cluster (according to the distance defined by the word embedding vector) obtained during the training part is assigned to the product. This process is done for all rows of the test data, and then, in the same way as in the training part, a one-hot encoded version of the DataFrame containing the product description and the corresponding cluster number is obtained.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error and the median absolute error (as the average of the results obtained for these two metrics over the 5 cross-validation folds) is then calculated for each prediction model. The results are analyzed after the lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "--\n",
      "Random Forest Regressor\n",
      "Median absolute error: 10.3695943096\n",
      "Mean absolute error: 28.2917930555\n",
      "--\n",
      "K Neighbors Regressor\n",
      "Median absolute error: 11.0048\n",
      "Mean absolute error: 31.3873834655\n",
      "--\n",
      "Decision Tree Regressor\n",
      "Median absolute error: 10.4463348686\n",
      "Mean absolute error: 28.9819960038\n",
      "--\n",
      "Linear Regression\n",
      "Median absolute error: 21.5956271431\n",
      "Mean absolute error: 39.2957860884\n",
      "--\n",
      "RANSAC Regressor\n",
      "Median absolute error: 15.2715519949\n",
      "Mean absolute error: 44.8184361425\n"
     ]
    }
   ],
   "source": [
    "kFoldMedianAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMedianAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMedianAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMedianAbsoluteErrorsLinearRegression = []\n",
    "kFoldMedianAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kFoldMeanAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMeanAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMeanAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMeanAbsoluteErrorsLinearRegression = []\n",
    "kFoldMeanAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kFoldNumber = 1\n",
    "kf = KFold(n_splits=5, random_state=1)\n",
    "for train_index, test_index in kf.split(outcomesDf):\n",
    "    #5-fold cross-validation\n",
    "    print(\"Executing k fold = \"+str(kFoldNumber))\n",
    "    kFoldNumber=kFoldNumber+1\n",
    "    \n",
    "    #train and test split\n",
    "    outcomesDf_train, outcomesDf_test = outcomesDf.iloc[train_index], outcomesDf.iloc[test_index]   \n",
    "    \n",
    "    #TRAINING PART\n",
    "    \n",
    "    #Unique product descriptions present in the training dataset\n",
    "    productDescriptionTrain = outcomesDf_train['desc'].unique()\n",
    "    \n",
    "    productDescriptionToVector={}\n",
    "    for item in productDescriptionTrain:\n",
    "        #Word2Vec vector obtained for each product description\n",
    "        productDescriptionToVector[item] = productDescriptionToVectorDf[item]\n",
    "    \n",
    "    productVectors = list(productDescriptionToVector.values())\n",
    "   \n",
    "    km = KMeans(n_clusters=20,random_state=2)\n",
    "    #the clustering is performed with the Word2Vec vectors for the product categories given as input\n",
    "    km.fit(productVectors)\n",
    "    clusters = km.labels_.tolist()\n",
    "    #DataFrame containing columns for the product description and the category cluster that it is associated to\n",
    "    clusteredCategoriesDf = pd.DataFrame({'desc': list(productDescriptionToVector.keys()), 'cat_cluster': clusters})\n",
    "    #One-Hot encoded version of the DataFrame containing a column for each cluster\n",
    "    clusteredCategoriesDfOneHotEnc = convertCategoryDataFrameToOneHotEncodedVersion(clusteredCategoriesDf,'cat_cluster')  \n",
    "    #column names of the clusters\n",
    "    cluster_column_names = clusteredCategoriesDfOneHotEnc.columns.values\n",
    "    clusteredCategoriesDfOneHotEnc['desc'] = clusteredCategoriesDf['desc']\n",
    "    #the training dataset is merged so that it contains a one hot-encoded \n",
    "    #column for each one of the clusters obtained.\n",
    "    outcomesDfWithCatOneHot = outcomesDf_train.merge(clusteredCategoriesDfOneHotEnc,how='left')\n",
    "    normal_x_column_names = [\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]\n",
    "    #column names to be used in the prediction model: they include the columns corresponding to the clusters obtained.\n",
    "    column_names_with_clusters = np.concatenate([normal_x_column_names,cluster_column_names])\n",
    "    \n",
    "    #training phase variables\n",
    "    X_train = outcomesDfWithCatOneHot[column_names_with_clusters]\n",
    "    y_train = outcomesDfWithCatOneHot[\"price\"]\n",
    "    \n",
    "    #TEST PART\n",
    "    \n",
    "    #Unique product descriptions present in the training dataset\n",
    "    productDescriptionTest = outcomesDf_test['desc'].unique()\n",
    "    \n",
    "    productDescriptionToVector={}\n",
    "    for item in productDescriptionTest:\n",
    "        #Word2Vec vector obtained for each product description\n",
    "        productDescriptionToVector[item] = productDescriptionToVectorDf[item]\n",
    "   \n",
    "    productDescriptionToTrainCluster = {}\n",
    "    for productDescription, vector in productDescriptionToVector.items():\n",
    "        #The K-Means model obtained during the training phase is used to associate\n",
    "        #an existing cluster to each one of the product vectors contained in the test data.\n",
    "        clusterCategory = km.predict([vector])\n",
    "        productDescriptionToTrainCluster[productDescription] = clusterCategory[0]\n",
    "    \n",
    "    #DataFrame containing columns for the product description and the category cluster that it is associated to\n",
    "    clusteredCategoriesDf = pd.DataFrame(list(productDescriptionToTrainCluster.items()), columns=['desc', 'cat_cluster'])\n",
    "    #One-Hot encoded version of the DataFrame containing a column for each cluster.\n",
    "    #The columns are the same ones as in the training phase\n",
    "    clusteredCategoriesDfOneHotEnc = convertCategoryDataFrameToOneHotEncodedVersion(clusteredCategoriesDf,'cat_cluster',cluster_column_names)\n",
    "    #column names of the clusters\n",
    "    cluster_column_names = clusteredCategoriesDfOneHotEnc.columns.values\n",
    "    clusteredCategoriesDfOneHotEnc['desc'] = clusteredCategoriesDf['desc']\n",
    "    #the test dataset is merged so that it contains a one hot-encoded \n",
    "    #column for each one of the clusters obtained.\n",
    "    outcomesDfWithCatOneHot = outcomesDf_test.merge(clusteredCategoriesDfOneHotEnc,how='left')\n",
    "    normal_x_column_names = [\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]\n",
    "    #column names to be used in the prediction model: they include the columns corresponding to the clusters obtained.\n",
    "    column_names_with_clusters = np.concatenate([normal_x_column_names,cluster_column_names])\n",
    "    \n",
    "    #test phase variables\n",
    "    X_test = outcomesDfWithCatOneHot[column_names_with_clusters]\n",
    "    y_test = outcomesDfWithCatOneHot[\"price\"]\n",
    "    \n",
    "    #Predictors\n",
    "    \n",
    "    #RandomForestRegressor\n",
    "    model=RandomForestRegressor(random_state=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRandomForestRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRandomForestRegressor.append(kFoldMeanAbsoluteError)  \n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    model = KNeighborsRegressor()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsKNeighborsRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsKNeighborsRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #DecisionTreeRegressor\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsDecisionTreeRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsDecisionTreeRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsLinearRegression.append(kFoldMedianAbsoluteError) \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsLinearRegression.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #RANSACRegressor\n",
    "    model = RANSACRegressor(random_state=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRANSACRegressor.append(kFoldMedianAbsoluteError) \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRANSACRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "medianAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRandomForestRegressor)\n",
    "meanAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRandomForestRegressor)\n",
    "medianAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsKNeighborsRegressor)\n",
    "meanAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsKNeighborsRegressor)\n",
    "medianAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsDecisionTreeRegressor)\n",
    "meanAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsDecisionTreeRegressor)\n",
    "medianAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMedianAbsoluteErrorsLinearRegression)\n",
    "meanAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMeanAbsoluteErrorsLinearRegression)\n",
    "medianAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRANSACRegressor)\n",
    "meanAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRANSACRegressor)\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Random Forest Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"K Neighbors Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Decision Tree Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Linear Regression\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"--\")\n",
    "print(\"RANSAC Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRANSACRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRANSACRegressorAverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the model with the best results is the random forest regressor, both in terms of the median absolute error and the mean absolute error. It is closely followed by the decision tree regressor. Although the random forest regressor performs better in terms of the chosen metrics, the decision tree regressor is easier to interpret. Nevertheless, the decision tree regressor is also more likely to overfit the data. The k-neighbors regressor is the next one with the better results, followed by the RANSAC regressor. Ultimatelly, the worst results are obtained with the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the results are better with this set of models that takes into account the product categories as compared with the results obtained with the set of models that does not take them into account (explained in the previous section).\n",
    "\n",
    "Nevertheless, this set of models requires a bigger effort, since a preprocessing step is necessary to make predictions. This preprocessing step consists on, given the product description of an item in the column \"desc\", its Amazon category must be obtained, and afterwards, the corresponding word embedding vector.\n",
    "\n",
    "Furthermore, during the training part of the model, the product category clusters are obtained, and these clusters are the ones that are used each time that a new prediction is made. After some time, more and more new products will begin to be auctioned, which will not have been considered to form the clusters. Because of this, the clusters may eventually become outdated, and the models will perform worse. Therefore, appart from the preprocessing step, using the product categories also involves having to retrain the prediction models more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Multiple models - Clustering by product categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third set of models that have into account the product categories has been built. The difference between this set of models and the previous one is that instead of using a single model for all items, a different prediction model is used for each different product category (i.e., each cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomesDf = pd.read_csv('./outcomes_clean.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDescriptionToVectorDf = pd.read_excel('./productDescriptionToVector.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as in the previous set of models, the following lines of code calculate the metrics associated to each one of the prediction models using 5-fold cross-validation. During the training part, the K-Means algorithm is used to obtain clusters for the word embedding vectors. Once that each different item in the training data has been assigned to a cluster, the training data is divided by cluster number. For each set of data belonging to the same cluster, a new set of prediction models is trained. The input variables for the model are the retail price of the item, the bid increment, the bid fee and the flags that indicate the type of auction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the test part, for each row contained in the test data, the word embedding vector associated to the product description is obtained. Then, the nearest cluster (according to the distance defined by the word embedding vector) obtained during the training part is assigned to the product. For each row contained in the test data, the corresponding set of models for that product category (cluster) is used to make the selling price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error and the median absolute error (as the average of the results obtained for these two metrics over the 5 cross-validation folds) is then calculated for each prediction model contained in the different sets assigned to each cluster. For each type of prediction model (random forest regressor, decision tree regressor, etc), the mean absolute error and the median absolute error is calculated as the average of the results obtained for each one of the models (with the same type of prediction algorithm) corresponding to each different cluster. The results are analyzed after the lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "--\n",
      "Random Forest Regressor\n",
      "Median absolute error: 10.3682931183\n",
      "Mean absolute error: 28.429692278\n",
      "--\n",
      "K Neighbors Regressor\n",
      "Median absolute error: 11.13\n",
      "Mean absolute error: 30.7647559882\n",
      "--\n",
      "Decision Tree Regressor\n",
      "Median absolute error: 10.4300171004\n",
      "Mean absolute error: 28.9224769627\n",
      "--\n",
      "Linear Regression\n",
      "Median absolute error: 16.4468950152\n",
      "Mean absolute error: 33.9439732817\n",
      "--\n",
      "RANSAC Regressor\n",
      "Median absolute error: 9.99884623609\n",
      "Mean absolute error: 32.1491303303\n"
     ]
    }
   ],
   "source": [
    "kFoldMedianAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMedianAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMedianAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMedianAbsoluteErrorsLinearRegression = []\n",
    "kFoldMedianAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kFoldMeanAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMeanAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMeanAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMeanAbsoluteErrorsLinearRegression = []\n",
    "kFoldMeanAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=1)\n",
    "kFoldNumber = 1\n",
    "for train_index, test_index in kf.split(outcomesDf):\n",
    "    #5-fold cross-validation\n",
    "    print(\"Executing k fold = \"+str(kFoldNumber))\n",
    "    kFoldNumber=kFoldNumber+1\n",
    "    \n",
    "    #train and test split\n",
    "    outcomesDf_train, outcomesDf_test = outcomesDf.iloc[train_index], outcomesDf.iloc[test_index]   \n",
    "    \n",
    "    #TRAINING PART\n",
    "                                        \n",
    "    #Unique product descriptions present in the training dataset\n",
    "    productDescriptionTrain = outcomesDf_train['desc'].unique()\n",
    "    \n",
    "    productDescriptionToVector={}\n",
    "    for item in productDescriptionTrain:\n",
    "        #Word2Vec vector obtained for each product description\n",
    "        productDescriptionToVector[item] = productDescriptionToVectorDf[item]\n",
    "        \n",
    "    productVectors = list(productDescriptionToVector.values())\n",
    "    \n",
    "    km = KMeans(n_clusters=15,random_state=2)\n",
    "    #the clustering is performed with the Word2Vec vectors for the product categories given as input\n",
    "    km.fit(productVectors)\n",
    "    clusters = km.labels_.tolist()\n",
    "    #DataFrame containing columns for the product description and the category cluster that it is associated to\n",
    "    clusteredCategoriesDf = pd.DataFrame({'desc': list(productDescriptionToVector.keys()), 'cat_cluster': clusters})\n",
    "    #the training dataset is merged and now it contains a column with the category that\n",
    "    #each product belongs to.\n",
    "    outcomesDfTrainAndCatCluster = outcomesDf_train.merge(clusteredCategoriesDf,how='left')\n",
    "    \n",
    "    clusterIndexToOutcomesDfTrain = {}\n",
    "    for cluster_index in outcomesDfTrainAndCatCluster['cat_cluster'].unique():\n",
    "        #the rows associated to each different product category are stored in a dictionary,\n",
    "        #where the dictionary key is the product category number (cluster number)\n",
    "        clusterIndexToOutcomesDfTrain[cluster_index] = outcomesDfTrainAndCatCluster[outcomesDfTrainAndCatCluster['cat_cluster'] == cluster_index]\n",
    "    \n",
    "    #dictionaries that contain a trained model for each one of the product categories (clusters)\n",
    "    clusterIndexToRandomForestRegressor = {}\n",
    "    clusterIndexToKNeighborsRegressor = {}\n",
    "    clusterIndexToDecisionTreeRegressor = {}\n",
    "    clusterIndexToLinearRegression = {}\n",
    "    clusterIndexToRANSACRegressor = {}\n",
    "    \n",
    "    for cluster_index, outcomesDfClusterIndex in clusterIndexToOutcomesDfTrain.items():\n",
    "        #For each product category (cluster), a different model is trained using\n",
    "        #the rows of the input dataset containing the products associated to that category\n",
    "        X_train = outcomesDfClusterIndex[[\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]].values\n",
    "        y_train = outcomesDfClusterIndex[\"price\"]\n",
    "        \n",
    "        #RandomForestRegressor\n",
    "        model=RandomForestRegressor(random_state=1)\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToRandomForestRegressor[cluster_index] = model\n",
    "\n",
    "        #KNeighborsRegressor\n",
    "        n_neighbors=5\n",
    "        if outcomesDfClusterIndex.shape[0] < n_neighbors:\n",
    "            #In KNeighborsRegressor it is expected n_neighbors <= n_samples\n",
    "            n_neighbors = outcomesDfClusterIndex.shape[0]\n",
    "            \n",
    "        model = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToKNeighborsRegressor[cluster_index] = model\n",
    "\n",
    "        #DecisionTreeRegressor\n",
    "        model = DecisionTreeRegressor()\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToDecisionTreeRegressor[cluster_index] = model\n",
    "    \n",
    "        #LinearRegression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToLinearRegression[cluster_index] = model\n",
    "        \n",
    "        #RANSACRegressor\n",
    "        model = RANSACRegressor(random_state=1)\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToRANSACRegressor[cluster_index] = model\n",
    "        \n",
    "    #TEST PART\n",
    "    \n",
    "    #Unique product descriptions present in the test dataset\n",
    "    productDescriptionTest = outcomesDf_test['desc'].unique()\n",
    "    \n",
    "    productDescriptionToVector={}\n",
    "    for item in productDescriptionTest:\n",
    "        #Word2Vec vector obtained for each product description\n",
    "        productDescriptionToVector[item] = productDescriptionToVectorDf[item]\n",
    "   \n",
    "    productDescriptionToTrainCluster = {}\n",
    "    for productDescription, vector in productDescriptionToVector.items(): \n",
    "        #The K-Means model obtained during the training phase is used to associate\n",
    "        #an existing cluster to each one of the product vectors contained in the test data.        \n",
    "        clusterCategory = km.predict([vector])\n",
    "        productDescriptionToTrainCluster[productDescription] = clusterCategory[0]\n",
    "    \n",
    "    #DataFrame containing columns for the product description and the category cluster that it is associated to\n",
    "    clusteredCategoriesDf = pd.DataFrame(list(productDescriptionToTrainCluster.items()), columns=['desc', 'cat_cluster'])\n",
    "    #the test dataset is merged and now it contains a column with the category that\n",
    "    #each product belongs to.\n",
    "    outcomesDfTestAndCatCluster = outcomesDf_test.merge(clusteredCategoriesDf,how='left')\n",
    "    \n",
    "    realAndPredictedValuesRandomForestRegressor = []\n",
    "    realAndPredictedValuesKNeighborsRegressor = []\n",
    "    realAndPredictedValuesDecisionTreeRegressor = []\n",
    "    realAndPredictedValuesLinearRegression = []\n",
    "    realAndPredictedValuesRANSACRegressor = []\n",
    "    \n",
    "    for index, row in outcomesDfTestAndCatCluster.iterrows():\n",
    "        #iteration through each row of the test dataset\n",
    "        \n",
    "        #real value of the column to be predicted\n",
    "        y_test_value = row[\"price\"]\n",
    "        #values given as input for the prediction model\n",
    "        x_test_value = row[[\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]].values\n",
    "        #product category cluster for this row of the dataset\n",
    "        clusterIndex = row[\"cat_cluster\"]\n",
    "        \n",
    "        #the prediction models associated to this product category (cluster) are extracted\n",
    "        modelRandomForestRegressor = clusterIndexToRandomForestRegressor[clusterIndex]\n",
    "        modelKNeighborsRegressor = clusterIndexToKNeighborsRegressor[clusterIndex]\n",
    "        modelDecisionTreeRegressor = clusterIndexToDecisionTreeRegressor[clusterIndex]\n",
    "        modelLinearRegression = clusterIndexToLinearRegression[clusterIndex]\n",
    "        modelRANSACRegressor = clusterIndexToRANSACRegressor[clusterIndex]\n",
    "        \n",
    "        #the real value and the predicted value are stored in a different list for\n",
    "        #each one of the prediction models used.\n",
    "        realAndPredictedValuesRandomForestRegressor.append((y_test_value,modelRandomForestRegressor.predict([x_test_value])[0]))\n",
    "        realAndPredictedValuesKNeighborsRegressor.append((y_test_value,modelKNeighborsRegressor.predict([x_test_value])[0]))\n",
    "        realAndPredictedValuesDecisionTreeRegressor.append((y_test_value,modelDecisionTreeRegressor.predict([x_test_value])[0]))\n",
    "        realAndPredictedValuesLinearRegression.append((y_test_value,modelLinearRegression.predict([x_test_value])[0]))\n",
    "        realAndPredictedValuesRANSACRegressor.append((y_test_value,modelRANSACRegressor.predict([x_test_value])[0]))\n",
    "    \n",
    "    #RandomForestRegressor\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesRandomForestRegressor)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRandomForestRegressor.append(kFoldMedianAbsoluteError) \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRandomForestRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #KNeighborsRegressor\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesKNeighborsRegressor)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsKNeighborsRegressor.append(kFoldMedianAbsoluteError)  \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsKNeighborsRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #DecisionTreeRegressor\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesDecisionTreeRegressor)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsDecisionTreeRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsDecisionTreeRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #LinearRegression\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesLinearRegression)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsLinearRegression.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsLinearRegression.append(kFoldMeanAbsoluteError) \n",
    "    \n",
    "    #RANSACRegressor\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesRANSACRegressor)\n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRANSACRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRANSACRegressor.append(kFoldMeanAbsoluteError) \n",
    "    \n",
    "medianAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRandomForestRegressor)\n",
    "meanAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRandomForestRegressor)\n",
    "medianAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsKNeighborsRegressor)\n",
    "meanAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsKNeighborsRegressor)\n",
    "medianAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsDecisionTreeRegressor)\n",
    "meanAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsDecisionTreeRegressor)\n",
    "medianAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMedianAbsoluteErrorsLinearRegression)\n",
    "meanAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMeanAbsoluteErrorsLinearRegression)\n",
    "medianAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRANSACRegressor)\n",
    "meanAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRANSACRegressor)\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Random Forest Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"K Neighbors Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Decision Tree Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Linear Regression\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"--\")\n",
    "print(\"RANSAC Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRANSACRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRANSACRegressorAverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the model with the best results in terms of the median absolute error is the RANSAC regressor. Nevertheless, the random forest regressor performs similarly in terms of the median absolute error, and performs way better in terms of the mean absolute error. It is closely followed by the decision tree regresor, which is easier to interpret but is also more likely to overfit the data. The k-neighbors regressor is the next one with the better results, followed by the linear regression with the worst results by far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the results are slightly better with this set of models as compared with the set of models explained in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of effort, this model requires the same preprocessing step to make predictions as the previous set of models: given the product description of an item in the column \"desc\", its Amazon category must be obtained, and afterwards, the corresponding word embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different prediction model is used for each different product category (cluster) that is obtained during the training part of the model. In the same way as with the set of models mentioned in the previous section, more and more new products will begin to be auctioned after some time, and the clusters (and therefore, the model assigned to each cluster) will become outdated and the models will perform worse. Therefore, appart from the preprocessing step, the prediction models have to be retrained after some time (and this time, it is not necessary to retrain a single model, but to retrain as many prediction models as existing clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision between using a single model to make the predictions (as explained in the previous section), or using a different model for each product category (as explained in this section) should also be highly influenced by the amount of training data available. The data that is used to train the prediction model corresponding to a single cluster is only the one that contains items associated to the product category corresponding to that cluster. If the amount of training data is low, the model will perform badly when making predictions for that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the metrics have been calculated as the average results between the models corresponding to the different clusters. Therefore, it may happen that the prediction model associated to a certain cluster performs much worse than the ones associated to the other clusters (because the amount of training data for that cluster is low)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, using a different prediction model for each cluster may be interesting when the amount of training data is big, but if this is not the case, it is better to use a single prediction model for all clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d) Single model - Clustering by product categories and retail prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fourth set of models that have into account the product categories have been built. In this case, a single prediction model is used to make the predictions. The difference is that the product category clusters have not only been built with the word embedding vector representing the product category, but also with the retail price of the item.\n",
    "\n",
    "This has been done to divide the products not only by their category, but also by their retail price. For example, mobile phone auctions can consist on high-end and mid-range mobile phones. The product category in both cases is mobile phones, but the selling price of high-end mobile phones will likely be higher than for mid-range mobile phones. Therefore, if different clusters are created for these two cases, the prediction model may perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomesDf = pd.read_csv('./outcomes_clean.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDescriptionToVectorDf = pd.read_excel('./productDescriptionToVector.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the same product can be auctioned more than once. The retail price of the same item in different auctions can also change. For example, the retail price of this external hard drive changes over time: it starts with 169.99\\$ in the first auctions in the dataset, and then changes to 79.00\\$, then 86.26\\$ and ends up having a retail price of 94.99\\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>retail</th>\n",
       "      <th>endtime_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-27 07:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-26 08:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-22 07:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-23 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-24 07:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-25 06:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-28 06:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-05 06:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-04 06:29:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-03 07:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5743</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-02 03:14:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5744</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-29 03:19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5745</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-09-30 03:29:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5746</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-01 03:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5747</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-13 15:22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9548</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-14 15:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9549</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-15 15:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9550</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-17 11:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9551</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-17 23:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9552</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-19 00:09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9553</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-31 06:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9554</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-11-01 01:14:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10158</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-26 12:52:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10159</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-24 18:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10160</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-24 11:57:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10161</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-22 16:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10162</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-22 07:48:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10163</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-20 15:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10164</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-10-26 16:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12251</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>169.99</td>\n",
       "      <td>2008-11-02 05:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111094</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-01 17:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111095</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-08 15:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111096</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-07 15:09:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111097</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-06 15:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111098</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-05 15:14:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111099</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-04 15:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111100</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-03 15:06:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113974</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-13 07:34:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113975</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-12 07:26:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113976</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-11 07:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113977</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-10 07:21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113978</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-09 14:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113979</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-14 19:53:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113980</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>79.00</td>\n",
       "      <td>2009-11-13 20:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116048</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-16 02:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116049</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-17 02:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116050</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>94.99</td>\n",
       "      <td>2009-11-18 02:07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116051</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>94.99</td>\n",
       "      <td>2009-11-19 02:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116052</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>94.99</td>\n",
       "      <td>2009-11-20 02:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116053</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>94.99</td>\n",
       "      <td>2009-11-21 02:28:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116054</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>94.99</td>\n",
       "      <td>2009-11-22 02:46:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117698</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-22 21:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117699</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-23 21:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117700</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-24 21:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117701</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-25 21:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117702</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-26 21:24:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117703</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-27 21:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117704</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>86.26</td>\n",
       "      <td>2009-11-28 21:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119398</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>94.99</td>\n",
       "      <td>2009-11-30 05:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119399</th>\n",
       "      <td>Western Digital My Passport Essential 320GB Blue</td>\n",
       "      <td>94.99</td>\n",
       "      <td>2009-12-01 05:44:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    desc  retail  \\\n",
       "2163    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2164    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2165    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2166    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2167    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2168    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2169    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2170    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2171    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "2172    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "5743    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "5744    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "5745    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "5746    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "5747    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "9548    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "9549    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "9550    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "9551    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "9552    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "9553    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "9554    Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "10158   Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "10159   Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "10160   Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "10161   Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "10162   Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "10163   Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "10164   Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "12251   Western Digital My Passport Essential 320GB Blue  169.99   \n",
       "...                                                  ...     ...   \n",
       "111094  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "111095  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "111096  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "111097  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "111098  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "111099  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "111100  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "113974  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "113975  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "113976  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "113977  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "113978  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "113979  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "113980  Western Digital My Passport Essential 320GB Blue   79.00   \n",
       "116048  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "116049  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "116050  Western Digital My Passport Essential 320GB Blue   94.99   \n",
       "116051  Western Digital My Passport Essential 320GB Blue   94.99   \n",
       "116052  Western Digital My Passport Essential 320GB Blue   94.99   \n",
       "116053  Western Digital My Passport Essential 320GB Blue   94.99   \n",
       "116054  Western Digital My Passport Essential 320GB Blue   94.99   \n",
       "117698  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "117699  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "117700  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "117701  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "117702  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "117703  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "117704  Western Digital My Passport Essential 320GB Blue   86.26   \n",
       "119398  Western Digital My Passport Essential 320GB Blue   94.99   \n",
       "119399  Western Digital My Passport Essential 320GB Blue   94.99   \n",
       "\n",
       "                endtime_str  \n",
       "2163    2008-09-27 07:01:00  \n",
       "2164    2008-09-26 08:20:00  \n",
       "2165    2008-09-22 07:08:00  \n",
       "2166    2008-09-23 07:00:00  \n",
       "2167    2008-09-24 07:03:00  \n",
       "2168    2008-09-25 06:52:00  \n",
       "2169    2008-09-28 06:51:00  \n",
       "2170    2008-10-05 06:39:00  \n",
       "2171    2008-10-04 06:29:00  \n",
       "2172    2008-10-03 07:15:00  \n",
       "5743    2008-10-02 03:14:00  \n",
       "5744    2008-09-29 03:19:00  \n",
       "5745    2008-09-30 03:29:00  \n",
       "5746    2008-10-01 03:37:00  \n",
       "5747    2008-10-13 15:22:00  \n",
       "9548    2008-10-14 15:50:00  \n",
       "9549    2008-10-15 15:12:00  \n",
       "9550    2008-10-17 11:56:00  \n",
       "9551    2008-10-17 23:32:00  \n",
       "9552    2008-10-19 00:09:00  \n",
       "9553    2008-10-31 06:24:00  \n",
       "9554    2008-11-01 01:14:00  \n",
       "10158   2008-10-26 12:52:00  \n",
       "10159   2008-10-24 18:10:00  \n",
       "10160   2008-10-24 11:57:00  \n",
       "10161   2008-10-22 16:24:00  \n",
       "10162   2008-10-22 07:48:00  \n",
       "10163   2008-10-20 15:20:00  \n",
       "10164   2008-10-26 16:25:00  \n",
       "12251   2008-11-02 05:15:00  \n",
       "...                     ...  \n",
       "111094  2009-11-01 17:31:00  \n",
       "111095  2009-11-08 15:12:00  \n",
       "111096  2009-11-07 15:09:00  \n",
       "111097  2009-11-06 15:03:00  \n",
       "111098  2009-11-05 15:14:00  \n",
       "111099  2009-11-04 15:16:00  \n",
       "111100  2009-11-03 15:06:00  \n",
       "113974  2009-11-13 07:34:00  \n",
       "113975  2009-11-12 07:26:00  \n",
       "113976  2009-11-11 07:44:00  \n",
       "113977  2009-11-10 07:21:00  \n",
       "113978  2009-11-09 14:56:00  \n",
       "113979  2009-11-14 19:53:00  \n",
       "113980  2009-11-13 20:04:00  \n",
       "116048  2009-11-16 02:25:00  \n",
       "116049  2009-11-17 02:25:00  \n",
       "116050  2009-11-18 02:07:00  \n",
       "116051  2009-11-19 02:50:00  \n",
       "116052  2009-11-20 02:05:00  \n",
       "116053  2009-11-21 02:28:00  \n",
       "116054  2009-11-22 02:46:00  \n",
       "117698  2009-11-22 21:30:00  \n",
       "117699  2009-11-23 21:08:00  \n",
       "117700  2009-11-24 21:18:00  \n",
       "117701  2009-11-25 21:10:00  \n",
       "117702  2009-11-26 21:24:00  \n",
       "117703  2009-11-27 21:39:00  \n",
       "117704  2009-11-28 21:18:00  \n",
       "119398  2009-11-30 05:55:00  \n",
       "119399  2009-12-01 05:44:00  \n",
       "\n",
       "[243 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomesDf[outcomesDf['desc'] == \"Western Digital My Passport Essential 320GB Blue\"][['desc','retail','endtime_str']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it is the same hard drive, an item that was before considered a high-end item might become obsolote in terms of technology as time passes by, and end up being considered a mid-range item and finally a low-cost item. The evolution of the retail price of the item over time is a good indicator of this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as in the set of models explained before where clusters were obtained for the product categories, the objective of this clustering is to provide the prediction model with a grouping of the items, so that it is easier for the model to find a pattern (if it exists) within products belonging to the same group. In the cases in which the retail price of the same product changes throughout the different auctions, each unique combination of the product description and retail price has been considered as a different item.\n",
    "\n",
    "Duplicated combinations of product description and retail prices have been removed from the input data given to the clustering algorithm so that each different product has the same weight for the clustering. This way, the same product might be associated to a different cluster according to the retail price that it has at a certain point of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 1769 unique products (according to their product description) contained in the dataset, 1298 products have only one retail price over time, 307 products have two different retail prices, 135 products have three different retail prices, 25 products have four different retail prices, 2 products have five different retail prices, 1 product has six different retail prices, and 1 product has 14 different retail prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1769"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outcomesDf['desc'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     1298\n",
       "2      307\n",
       "3      135\n",
       "4       25\n",
       "5        2\n",
       "6        1\n",
       "14       1\n",
       "Name: desc_count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame containing unique combinations of the columns \"desc\" and \"retail\" (the same product\n",
    "#is auctioned more than once over time, and sometimes it is associated to a different retail price)\n",
    "descAndRetailAllPricesCombinations = outcomesDf.groupby(['desc','retail']).size().reset_index()\n",
    "descAndRetailAllPricesCombinations = descAndRetailAllPricesCombinations.drop(0,axis=1)\n",
    "#count of unique different retail prices for each unique product description\n",
    "countOfDifferentRetailPricesOverTime = descAndRetailAllPricesCombinations['desc'].value_counts()\n",
    "descAndRetailAllPricesCombinations = descAndRetailAllPricesCombinations.merge(countOfDifferentRetailPricesOverTime.to_frame(),how='left',left_on='desc',right_index=True)\n",
    "descAndRetailAllPricesCombinations = descAndRetailAllPricesCombinations.rename(columns={'desc_x': 'desc', 'desc_y': 'desc_count'})\n",
    "\n",
    "descAndRetailAllPricesCombinationsCount = descAndRetailAllPricesCombinations.groupby(['desc','desc_count']).size().reset_index()\n",
    "descAndRetailAllPricesCombinationsCount = descAndRetailAllPricesCombinationsCount.drop(0,axis=1)\n",
    "descAndRetailAllPricesCombinationsCount['desc_count'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training part, the K-Means algorithm is used to obtain the clusters. The input given to the clustering algorithm is the word embedding vector and the retail price of each different product (where, as explained above, unique combinations of the same product description with a different retail price counts as a different product)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding vector length is 300 features, and in total, there are 1769 word embedding vectors (unique products):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1769)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "productDescriptionToVectorDf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics for all values contained in all word embedding products are shown below. The values range approximately from -0.5 to 0.5, and the mean and median of the values are approximately zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value: -0.494140625\n",
      "Maximum value: 0.50732421875\n",
      "Mean value: -0.008482887480809824\n",
      "Median value: -0.007725306919642857\n",
      "Standard deviation: 0.09389948714274118\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum value: \"+str(productDescriptionToVectorDf.stack().min()))\n",
    "print(\"Maximum value: \"+str(productDescriptionToVectorDf.stack().max()))\n",
    "print(\"Mean value: \"+str(productDescriptionToVectorDf.stack().mean()))\n",
    "print(\"Median value: \"+str(productDescriptionToVectorDf.stack().median()))\n",
    "print(\"Standard deviation: \"+str(productDescriptionToVectorDf.stack().std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retail prices of the items are much higher, and therefore, if they are not scaled accordingly, they would have a higher influence when forming the clusters than the product categories. A Min-Max scaler could be used to scale the retail prices (of the unique combinations of product descriptions and retail prices) between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "descAndRetailPricesCombinations = outcomesDf.groupby(['desc','retail']).size().reset_index()\n",
    "descAndRetailPricesCombinations = descAndRetailPricesCombinations.drop(0,axis=1)\n",
    "#min-max scaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "#retail prices now range from 0 to 1\n",
    "retail_std = scaler.fit_transform(pd.DataFrame(descAndRetailPricesCombinations['retail']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, the mean and median that are obtained after scaling are very close to the minimum value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value: 0.0\n",
      "Maximum value: 1.0\n",
      "Mean value: 0.0155919137861\n",
      "Median value: 0.00610957230143\n",
      "Standard deviation: 0.0277558148788\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum value: \"+str(np.min(retail_std)))\n",
    "print(\"Maximum value: \"+str(np.max(retail_std)))\n",
    "print(\"Mean value: \"+str(np.mean(retail_std)))\n",
    "print(\"Median value: \"+str(np.median(retail_std)))\n",
    "print(\"Standard deviation: \"+str(np.std(retail_std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the Min-Max scaler is very sensitive to the presence of outliers. While the median of the retail prices of all auctioned items is approximately 90\\$, the maximum retail price found in the dataset corresponds to a car with a retail price of 24550\\$. Therefore, a robust scaler is more appropiate in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.989999999999995"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(outcomesDf['retail'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auction_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>item</th>\n",
       "      <th>desc</th>\n",
       "      <th>retail</th>\n",
       "      <th>price</th>\n",
       "      <th>finalprice</th>\n",
       "      <th>bidincrement</th>\n",
       "      <th>bidfee</th>\n",
       "      <th>winner</th>\n",
       "      <th>...</th>\n",
       "      <th>freebids</th>\n",
       "      <th>endtime_str</th>\n",
       "      <th>flg_click_only</th>\n",
       "      <th>flg_beginnerauction</th>\n",
       "      <th>flg_fixedprice</th>\n",
       "      <th>flg_endprice</th>\n",
       "      <th>bids_placed</th>\n",
       "      <th>swoopo_sale_price</th>\n",
       "      <th>swoopo_profit</th>\n",
       "      <th>winner_benefit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95421</th>\n",
       "      <td>217223</td>\n",
       "      <td>10013607</td>\n",
       "      <td>2009-mini-cooper-chili-red-and-black-con</td>\n",
       "      <td>2009 Mini Cooper Chili Red and Black Convertible</td>\n",
       "      <td>24550.0</td>\n",
       "      <td>3939.36</td>\n",
       "      <td>3939.36</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.6</td>\n",
       "      <td>CaCO3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-09-07 15:08:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32828.0</td>\n",
       "      <td>22739.251441</td>\n",
       "      <td>-1810.748559</td>\n",
       "      <td>19017.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       auction_id  product_id                                      item  \\\n",
       "95421      217223    10013607  2009-mini-cooper-chili-red-and-black-con   \n",
       "\n",
       "                                                   desc   retail    price  \\\n",
       "95421  2009 Mini Cooper Chili Red and Black Convertible  24550.0  3939.36   \n",
       "\n",
       "       finalprice  bidincrement  bidfee winner       ...        freebids  \\\n",
       "95421     3939.36          0.12     0.6  CaCO3       ...               0   \n",
       "\n",
       "               endtime_str flg_click_only  flg_beginnerauction  \\\n",
       "95421  2009-09-07 15:08:00              0                    0   \n",
       "\n",
       "       flg_fixedprice  flg_endprice  bids_placed  swoopo_sale_price  \\\n",
       "95421               0             0      32828.0       22739.251441   \n",
       "\n",
       "       swoopo_profit  winner_benefit  \n",
       "95421   -1810.748559        19017.64  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomesDf.sort_values(by='retail',ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the histogram of the retail prices corresponding to the unique combinations of product description and retail prices. As it can be seen, the distribution is right skewed and most of the retail prices range up to 500\\$. The retail prices above 500\\$ can be considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fe06e685400>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/EAAAFTCAYAAABxioxZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe06e566cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "\n",
    "plt.xticks(np.arange(0, 3000, 50),rotation=90)\n",
    "plt.hist(descAndRetailPricesCombinations['retail'], color = 'blue', edgecolor = 'black', bins = 2000)\n",
    "plt.xlim(0,3000)\n",
    "plt.xlabel('Retail price')\n",
    "plt.ylabel('Number of items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retail price of 500\\$ is reached at the 75% percentile of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499.97000000000003"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(descAndRetailPricesCombinations['retail'],75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following robust scaler uses a similar method to the Min-Max scaler, but instead of the maximum and minimum values, it uses percentiles instead, and so it is robust to outliers because it is not influenced by a few number of very large marginal outliers. In this case, the distribution is right skewed and the outliers have been considered to be at the 75% percentile, so the percentiles have been set to 0% and 75%. Of course, this means that it is using less data for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "descAndRetailPricesCombinations = outcomesDf.groupby(['desc','retail']).size().reset_index()\n",
    "descAndRetailPricesCombinations = descAndRetailPricesCombinations.drop(0,axis=1)\n",
    "#robust scaler\n",
    "scaler = preprocessing.RobustScaler(quantile_range=(0.0, 75.0))\n",
    "retail_std = scaler.fit_transform(pd.DataFrame(descAndRetailPricesCombinations['retail']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the median of the scaled prices is zero, in the same way as with the word embedding vectors. Note that the outliers themselves are still present in the transformed data, and so, the minimum and maximum values are not 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value: -0.29999799988\n",
      "Maximum value: 48.8029481769\n",
      "Mean value: 0.465610903552\n",
      "Median value: 0.0\n",
      "Standard deviation: 1.36289228409\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum value: \"+str(np.min(retail_std)))\n",
    "print(\"Maximum value: \"+str(np.max(retail_std)))\n",
    "print(\"Mean value: \"+str(np.mean(retail_std)))\n",
    "print(\"Median value: \"+str(np.median(retail_std)))\n",
    "print(\"Standard deviation: \"+str(np.std(retail_std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word embedding vector length is 300 features. Therefore, the scaled retail prices values have to be multiplied by 300 so that they have a similar weight as the word embedding vectors during the cluster calculation process. \n",
    "\n",
    "Since the robust scaler percentiles have been defined as 0% and 75%, only 75% of the data will have a similar weight as the word embedding vectors during the cluster calculation process. The other 25% of data has been considered to contain outliers during the scaling and will have a higher weight than the word embedding vectors during the cluster calculation process. Therefore, some clusters will be formed almost exclusively by the condition that the retail prices of the items belonging to it are extremely high. This behaviour is alright, since items with very high retail prices will most likely reach very high selling prices accordingly, independently of the product category that they belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as in the previous set of models, the following lines of code calculate the metrics associated to each one of the prediction models using 5-fold cross-validation. During the training part, the retail prices are scaled and the K-Means algorithm is used to obtain clusters for the unique combinations of word embedding vectors and scaled retail prices.\n",
    "\n",
    "Once that each different item has been assigned a cluster, a one-hot encoded version of the corresponding cluster numbers is added to the training data. The input variables for the prediction models are the retail price of the item, the bid increment, the bid fee, the flags that indicate the type of auction, and the one-hot encoded cluster identifier.\n",
    "\n",
    "During the test part, the retail prices of the unique combinations of product descriptions and retail prices have to be scaled too. The same scaler instance obtained during the training part is used on the test data to transform it in the same way (with the statistics calculated for the samples in the training set). Then, the nearest cluster obtained during the training part is assigned to each row in the test data. Once that each different row has been assigned a cluster, a one-hot encoded version of the corresponding cluster identifiers is added to the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error and the median absolute error (as the average of the results obtained for these two metrics over the 5 cross-validation folds) is then calculated for each prediction model. The results are analyzed after the lines of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "--\n",
      "Random Forest Regressor\n",
      "Median absolute error: 11.1213084905\n",
      "Mean absolute error: 32.5591795213\n",
      "--\n",
      "K Neighbors Regressor\n",
      "Median absolute error: 11.6736\n",
      "Mean absolute error: 31.6176039332\n",
      "--\n",
      "Decision Tree Regressor\n",
      "Median absolute error: 11.1622118506\n",
      "Mean absolute error: 30.0645029178\n",
      "--\n",
      "Linear Regression\n",
      "Median absolute error: 88.90648655\n",
      "Mean absolute error: 146.484095711\n",
      "--\n",
      "RANSAC Regressor\n",
      "Median absolute error: 38.3213749265\n",
      "Mean absolute error: 63.4435851497\n"
     ]
    }
   ],
   "source": [
    "kFoldMedianAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMedianAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMedianAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMedianAbsoluteErrorsLinearRegression = []\n",
    "kFoldMedianAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kFoldMeanAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMeanAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMeanAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMeanAbsoluteErrorsLinearRegression = []\n",
    "kFoldMeanAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kFoldNumber = 1\n",
    "kf = KFold(n_splits=5, random_state=1)\n",
    "for train_index, test_index in kf.split(outcomesDf):\n",
    "    #5-fold cross-validation\n",
    "    print(\"Executing k fold = \"+str(kFoldNumber))\n",
    "    kFoldNumber=kFoldNumber+1\n",
    "    \n",
    "    #train and test split\n",
    "    outcomesDf_train, outcomesDf_test = outcomesDf.iloc[train_index], outcomesDf.iloc[test_index]   \n",
    "    \n",
    "    #TRAINING PART\n",
    "        \n",
    "    productDescriptionToVector={}\n",
    "    for item in outcomesDf['desc'].unique():\n",
    "        #Word2Vec vector obtained for each product description\n",
    "        productDescriptionToVector[item] = productDescriptionToVectorDf[item]\n",
    "    \n",
    "    #DataFrame containing unique combinations of the columns \"desc\" and \"retail\" (the same product\n",
    "    #is auctioned more than once over time, and sometimes it is associated to a different retail price)\n",
    "    descAndRetailPricesCombinationsTrain = outcomesDf_train.groupby(['desc','retail']).size().reset_index()\n",
    "    descAndRetailPricesCombinationsTrain = descAndRetailPricesCombinationsTrain.drop(0,axis=1)\n",
    "    \n",
    "    #robust scaler\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(0, 75.0))\n",
    "    retail_std = scaler.fit_transform(pd.DataFrame(descAndRetailPricesCombinationsTrain['retail']))\n",
    "    #Google's pre-trained Word2Vec model vector length is 300 features.\n",
    "    #Since the 300 features are given as input for the clustering, and the\n",
    "    #retail price has been scaled,the retail price is multiplied by 300\n",
    "    #so that it has the same importance for the clustering as the\n",
    "    #300 vector features altogether.\n",
    "    descAndRetailPricesCombinationsTrain['retail_std'] = retail_std*300\n",
    "    \n",
    "    productVectorColumnValuesAndRetail = []\n",
    "    for index, row in descAndRetailPricesCombinationsTrain.iterrows():\n",
    "        #iteration through each unique combination of the columns \"desc\" and \"retail\"\n",
    "        productDescription=row['desc']\n",
    "        auctionProductVector = np.array(productDescriptionToVector[productDescription])\n",
    "        #the Word2Vec vector for the product description and the retail price will be given\n",
    "        #as input for the clustering\n",
    "        productVectorColumnValuesAndRetail.append(np.append(auctionProductVector,row['retail_std']))\n",
    "    \n",
    "    km = KMeans(n_clusters=35,random_state=2)\n",
    "    #clustering with the Word2Vec vectors and retail prices given as input\n",
    "    km.fit(productVectorColumnValuesAndRetail)\n",
    "    clusters = km.labels_.tolist()\n",
    "    descAndRetailPricesCombinationsTrain['cat_cluster'] = clusters\n",
    "    \n",
    "    #One-Hot encoded version of the DataFrame containing a column for each cluster\n",
    "    clusteredCategoriesDfOneHotEnc = convertCategoryDataFrameToOneHotEncodedVersion(descAndRetailPricesCombinationsTrain,'cat_cluster')  \n",
    "    \n",
    "    #column names of the clusters\n",
    "    cluster_column_names = clusteredCategoriesDfOneHotEnc.columns.values\n",
    "    \n",
    "    descAndRetailPricesCombinationsTrain[clusteredCategoriesDfOneHotEnc.columns] = clusteredCategoriesDfOneHotEnc\n",
    "        \n",
    "    #the training dataset is merged so that it contains a one hot-encoded \n",
    "    #column for each one of the clusters obtained.\n",
    "    outcomesDfWithCatOneHot = outcomesDf_train.merge(descAndRetailPricesCombinationsTrain,how='left')\n",
    "    \n",
    "    normal_x_column_names = [\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]\n",
    "    #column names to be used in the prediction model: they include the columns corresponding to the clusters obtained.\n",
    "    column_names_with_clusters = np.concatenate([normal_x_column_names,cluster_column_names])\n",
    "    \n",
    "    #training phase variables\n",
    "    X_train = outcomesDfWithCatOneHot[column_names_with_clusters]\n",
    "    y_train = outcomesDfWithCatOneHot[\"price\"]\n",
    "    \n",
    "    #TEST PART\n",
    "    \n",
    "    #DataFrame containing unique combinations of the columns \"desc\" and \"retail\" (the same product\n",
    "    #is auctioned more than once over time, and sometimes it is associated to a different retail price)\n",
    "    descAndRetailPricesCombinationsTest = outcomesDf_test.groupby(['desc','retail']).size().reset_index()\n",
    "    descAndRetailPricesCombinationsTest = descAndRetailPricesCombinationsTest.drop(0,axis=1)\n",
    "    \n",
    "    #The scaler that is used in the test phase is the one that was obtained during the\n",
    "    #training phase.\n",
    "    #The same operations as in the training phase are performed.\n",
    "    retail_std = scaler.transform(pd.DataFrame(descAndRetailPricesCombinationsTest['retail']))\n",
    "    descAndRetailPricesCombinationsTest['retail_std'] = retail_std*300\n",
    "     \n",
    "    clusters = []\n",
    "    for index, row in descAndRetailPricesCombinationsTest.iterrows():\n",
    "        productDescription=row['desc']\n",
    "        auctionProductVector = np.array(productDescriptionToVector[productDescription])\n",
    "        productVectorColumnValuesAndRetail = np.append(auctionProductVector,row['retail_std'])\n",
    "        #The K-Means model obtained during the training phase is used to associate\n",
    "        #an existing cluster to each one of the combinations of product vectors and retail\n",
    "        #prices contained in the test data.   \n",
    "        clusterCategory = km.predict([productVectorColumnValuesAndRetail])\n",
    "        clusters.append(clusterCategory[0])\n",
    "        \n",
    "    descAndRetailPricesCombinationsTest['cat_cluster'] = clusters\n",
    "    \n",
    "    #One-Hot encoded version of the DataFrame containing a column for each cluster.\n",
    "    #The columns are the same ones as in the training phase\n",
    "    clusteredCategoriesDfOneHotEnc = convertCategoryDataFrameToOneHotEncodedVersion(descAndRetailPricesCombinationsTest,'cat_cluster',cluster_column_names)\n",
    "  \n",
    "    #column names of the clusters\n",
    "    cluster_column_names = clusteredCategoriesDfOneHotEnc.columns.values\n",
    "    \n",
    "    descAndRetailPricesCombinationsTest[clusteredCategoriesDfOneHotEnc.columns] = clusteredCategoriesDfOneHotEnc\n",
    "    \n",
    "    #the test dataset is merged so that it contains a one hot-encoded \n",
    "    #column for each one of the clusters obtained.\n",
    "    outcomesDfWithCatOneHot = outcomesDf_test.merge(descAndRetailPricesCombinationsTest,how='left')\n",
    "    \n",
    "    normal_x_column_names = [\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]\n",
    "    #column names to be used in the prediction model: they include the columns corresponding to the clusters obtained.\n",
    "    column_names_with_clusters = np.concatenate([normal_x_column_names,cluster_column_names])\n",
    "    \n",
    "    #test phase variables\n",
    "    X_test = outcomesDfWithCatOneHot[column_names_with_clusters]\n",
    "    y_test = outcomesDfWithCatOneHot[\"price\"]\n",
    "    \n",
    "    #Predictors\n",
    "    \n",
    "    #RandomForestRegressor\n",
    "    model=RandomForestRegressor(random_state=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRandomForestRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRandomForestRegressor.append(kFoldMeanAbsoluteError)  \n",
    "\n",
    "    #KNeighborsRegressor\n",
    "    model = KNeighborsRegressor()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsKNeighborsRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsKNeighborsRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #DecisionTreeRegressor\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsDecisionTreeRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsDecisionTreeRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsLinearRegression.append(kFoldMedianAbsoluteError) \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsLinearRegression.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "    #RANSACRegressor\n",
    "    model = RANSACRegressor(random_state=1)\n",
    "    model.fit(X_train,y_train)\n",
    "    P_price = model.predict(X_test)\n",
    "    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRANSACRegressor.append(kFoldMedianAbsoluteError) \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRANSACRegressor.append(kFoldMeanAbsoluteError)  \n",
    "    \n",
    "medianAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRandomForestRegressor)\n",
    "meanAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRandomForestRegressor)\n",
    "medianAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsKNeighborsRegressor)\n",
    "meanAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsKNeighborsRegressor)\n",
    "medianAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsDecisionTreeRegressor)\n",
    "meanAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsDecisionTreeRegressor)\n",
    "medianAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMedianAbsoluteErrorsLinearRegression)\n",
    "meanAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMeanAbsoluteErrorsLinearRegression)\n",
    "medianAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRANSACRegressor)\n",
    "meanAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRANSACRegressor)\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Random Forest Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"K Neighbors Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Decision Tree Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Linear Regression\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"--\")\n",
    "print(\"RANSAC Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRANSACRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRANSACRegressorAverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the model with the best results is the decision tree regressor. The random forest regressor and the the k-neighbors regressor perform slightly worse. Ultimately, the RANSAC regressor and the linear regression perform significantly worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the results are worse with this set of models as compared with the set of models explained in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of effort, this model requires several preprocessing steps: given the product description of an item in the column \"desc\", its Amazon category must be obtained, and afterwards, the corresponding word embedding vector. Moreover, the retail price of the item has to be scaled before making the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appart from the preprocessing steps, during the training part of the model, the clusters and the scaler instance are obtained, and they are later used everytime that a new prediction is made.  After some time, more and more new products will begin to be auctioned, which will not have been considered to form the clusters and to calculate the scaling values.  Because of this, the clusters and the scaler instance may eventually become outdated, and the models will perform worse. Therefore, appart from the preprocessing step, it is necessary to retrain the prediction models from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given dataset, this set of models is not interesting, since it is more complex and performs worse than some of the set of models explained in other sections. As the amount of data increases, it could be interesting to analyze the performance of this set of models again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e) Multiple models - Clustering by product categories and retail prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fith set of models that calculate clusters based on the product categories and retail prices has been built. The difference between this set of models and the previous one is that instead of using a single model for all items, a different prediction model is used for each different cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomesDf = pd.read_csv('./outcomes_clean.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDescriptionToVectorDf = pd.read_excel('./productDescriptionToVector.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as in the previous set of models, the following lines of code calculate the metrics associated to each one of the prediction models using 5-fold cross-validation. During the training part, the K-Means algorithm is used to obtain clusters for the unique combinations of product descriptions and retail prices. Once that each different item in the training data has been assigned to a cluster, the training data is divided by cluster identifier. For each set of data belonging to the same cluster, a new set of prediction models is trained. The input variables the retail price of the item, the bid increment, the bid fee and the flags that indicate the type of auction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the test part, the nearest cluster is assigned to each row contained in the test data. Then, the corresponding set of models corresponding to that cluster is used to make the selling price prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error and the median absolute error (as the average of the results obtained for these two metrics over the 5 cross-validation folds) is then calculated for each prediction model contained in the different sets assigned to each cluster. For each type of prediction model (random forest regressor, decision tree regressor, etc), the mean absolute error and the median absolute error is calculated as the average of the results obtained for each one of the models (with the same type of prediction algorithm) corresponding to each different cluster. The results are analyzed after the lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "--\n",
      "Random Forest Regressor\n",
      "Median absolute error: 11.0684002025\n",
      "Mean absolute error: 29.3948295166\n",
      "--\n",
      "K Neighbors Regressor\n",
      "Median absolute error: 11.7728\n",
      "Mean absolute error: 32.707053489\n",
      "--\n",
      "Decision Tree Regressor\n",
      "Median absolute error: 11.1026592998\n",
      "Mean absolute error: 29.8625242794\n",
      "--\n",
      "Linear Regression\n",
      "Median absolute error: 12.3694147353\n",
      "Mean absolute error: 32.0224389864\n",
      "--\n",
      "RANSAC Regressor\n",
      "Median absolute error: 10.6948726442\n",
      "Mean absolute error: 32.986749555\n"
     ]
    }
   ],
   "source": [
    "kFoldMedianAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMedianAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMedianAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMedianAbsoluteErrorsLinearRegression = []\n",
    "kFoldMedianAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kFoldMeanAbsoluteErrorsRandomForestRegressor = []\n",
    "kFoldMeanAbsoluteErrorsKNeighborsRegressor = []\n",
    "kFoldMeanAbsoluteErrorsDecisionTreeRegressor = []\n",
    "kFoldMeanAbsoluteErrorsLinearRegression = []\n",
    "kFoldMeanAbsoluteErrorsRANSACRegressor = []\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=1)\n",
    "kFoldNumber = 1\n",
    "for train_index, test_index in kf.split(outcomesDf):\n",
    "    #5-fold cross-validation\n",
    "    print(\"Executing k fold = \"+str(kFoldNumber))\n",
    "    kFoldNumber=kFoldNumber+1\n",
    "    \n",
    "    #train and test split\n",
    "    outcomesDf_train, outcomesDf_test = outcomesDf.iloc[train_index], outcomesDf.iloc[test_index]   \n",
    "    \n",
    "    #TRAINING PART\n",
    "    \n",
    "    productDescriptionToVector={}\n",
    "    for item in outcomesDf['desc'].unique():\n",
    "        #Word2Vec vector obtained for each product description\n",
    "        productDescriptionToVector[item] = productDescriptionToVectorDf[item]\n",
    "     \n",
    "    #DataFrame containing unique combinations of the columns \"desc\" and \"retail\" (the same product\n",
    "    #is auctioned more than once over time, and sometimes it is associated to a different retail price)\n",
    "    descAndRetailPricesCombinationsTrain = outcomesDf_train.groupby(['desc','retail']).size().reset_index()\n",
    "    descAndRetailPricesCombinationsTrain = descAndRetailPricesCombinationsTrain.drop(0,axis=1)\n",
    "    \n",
    "    #robust scaler\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(0, 75.0))\n",
    "    retail_std = scaler.fit_transform(pd.DataFrame(descAndRetailPricesCombinationsTrain['retail']))\n",
    "    \n",
    "    #Google's pre-trained Word2Vec model vector length is 300 features.\n",
    "    #Since the 300 features are given as input for the clustering, and the\n",
    "    #retail price has been scaled,the retail price is multiplied by 300\n",
    "    #so that it has the same importance for the clustering as the\n",
    "    #300 vector features altogether.\n",
    "    descAndRetailPricesCombinationsTrain['retail_std'] = retail_std*300\n",
    "       \n",
    "    productVectorColumnValuesAndRetail = []\n",
    "    for index, row in descAndRetailPricesCombinationsTrain.iterrows():\n",
    "        #iteration through each unique combination of the columns \"desc\" and \"retail\"\n",
    "        productDescription=row['desc']\n",
    "        auctionProductVector = np.array(productDescriptionToVector[productDescription])\n",
    "        #the Word2Vec vector for the product description and the retail price will be given\n",
    "        #as input for the clustering\n",
    "        productVectorColumnValuesAndRetail.append(np.append(auctionProductVector,row['retail_std']))\n",
    "    \n",
    "    km = KMeans(n_clusters=20,random_state=2)\n",
    "    #clustering with the Word2Vec vectors and retail prices given as input\n",
    "    km.fit(productVectorColumnValuesAndRetail)\n",
    "    clusters = km.labels_.tolist()\n",
    "    descAndRetailPricesCombinationsTrain['cat_cluster'] = clusters\n",
    "    #the training dataset is merged and now it contains a column with the cluster that\n",
    "    #each product belongs to.\n",
    "    outcomesDfTrainAndCatCluster = outcomesDf_train.merge(descAndRetailPricesCombinationsTrain,how='left')\n",
    "\n",
    "    clusterIndexToOutcomesDfTrain = {}\n",
    "    for cluster_index in outcomesDfTrainAndCatCluster['cat_cluster'].unique():\n",
    "        #the rows associated to each different cluster are stored in a dictionary,\n",
    "        #where the dictionary key is the cluster number\n",
    "        clusterIndexToOutcomesDfTrain[cluster_index] = outcomesDfTrainAndCatCluster[outcomesDfTrainAndCatCluster['cat_cluster'] == cluster_index]\n",
    "    \n",
    "    #dictionaries that contain a trained model for each one of the clusters\n",
    "    clusterIndexToRandomForestRegressor = {}\n",
    "    clusterIndexToKNeighborsRegressor = {}\n",
    "    clusterIndexToDecisionTreeRegressor = {}\n",
    "    clusterIndexToLinearRegression = {}\n",
    "    clusterIndexToRANSACRegressor = {}\n",
    "    \n",
    "    for cluster_index, outcomesDfClusterIndex in clusterIndexToOutcomesDfTrain.items():\n",
    "        #For each cluster, a different model is trained using\n",
    "        #the rows of the input dataset containing the products associated to that cluster\n",
    "        X_train = outcomesDfClusterIndex[[\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]].values\n",
    "        y_train = outcomesDfClusterIndex[\"price\"]\n",
    "        \n",
    "        #RandomForestRegressor\n",
    "        model=RandomForestRegressor(random_state=1)\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToRandomForestRegressor[cluster_index] = model\n",
    "\n",
    "        #KNeighborsRegressor\n",
    "        n_neighbors=5\n",
    "        if outcomesDfClusterIndex.shape[0] < n_neighbors:\n",
    "            #In KNeighborsRegressor it is expected n_neighbors <= n_samples\n",
    "            n_neighbors = outcomesDfClusterIndex.shape[0]\n",
    "            \n",
    "        model = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToKNeighborsRegressor[cluster_index] = model\n",
    "\n",
    "        #DecisionTreeRegressor\n",
    "        model = DecisionTreeRegressor()\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToDecisionTreeRegressor[cluster_index] = model\n",
    "    \n",
    "        #LinearRegression\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "        clusterIndexToLinearRegression[cluster_index] = model\n",
    "        \n",
    "        #RANSACRegressor\n",
    "        model = RANSACRegressor(random_state=1)\n",
    "        # assume linear model by default\n",
    "        min_samples = X_train.shape[1] + 1\n",
    "        if min_samples > X_train.shape[0]:\n",
    "            #min_samples may not be larger than number X_train.shape[0]\n",
    "            min_samples = X_train.shape[0]\n",
    "            model = RANSACRegressor(min_samples=min_samples,random_state=1)\n",
    "        try:\n",
    "            model.fit(X_train,y_train)\n",
    "            clusterIndexToRANSACRegressor[cluster_index] = model\n",
    "        except Exception as e: \n",
    "            continue\n",
    "        \n",
    "    #TEST PART\n",
    "    \n",
    "    #DataFrame containing unique combinations of the columns \"desc\" and \"retail\" (the same product\n",
    "    #is auctioned more than once over time, and sometimes it is associated to a different retail price)\n",
    "    descAndRetailPricesCombinationsTest = outcomesDf_test.groupby(['desc','retail']).size().reset_index()\n",
    "    descAndRetailPricesCombinationsTest = descAndRetailPricesCombinationsTest.drop(0,axis=1)\n",
    "    \n",
    "    #The scaler that is used in the test phase is the one that was obtained during the\n",
    "    #training phase.\n",
    "    #The same operations as in the training phase are performed.\n",
    "    retail_std = scaler.transform(pd.DataFrame(descAndRetailPricesCombinationsTest['retail']))\n",
    "    descAndRetailPricesCombinationsTest['retail_std'] = retail_std*300\n",
    "     \n",
    "    clusters = []\n",
    "    for index, row in descAndRetailPricesCombinationsTest.iterrows():\n",
    "        productDescription=row['desc']\n",
    "        auctionProductVector = np.array(productDescriptionToVector[productDescription])\n",
    "        productVectorColumnValuesAndRetail = np.append(auctionProductVector,row['retail_std'])\n",
    "        #The K-Means model obtained during the training phase is used to associate\n",
    "        #an existing cluster to each one of the combinations of product vectors and retail\n",
    "        #prices contained in the test data.   \n",
    "        clusterCategory = km.predict([productVectorColumnValuesAndRetail])\n",
    "        clusters.append(clusterCategory[0])\n",
    "        \n",
    "    descAndRetailPricesCombinationsTest['cat_cluster'] = clusters\n",
    "    #the test dataset is merged and now it contains a column with the cluster that\n",
    "    #each product belongs to.\n",
    "    outcomesDfTestAndCatCluster = outcomesDf_test.merge(descAndRetailPricesCombinationsTest,how='left')\n",
    "    \n",
    "    realAndPredictedValuesRandomForestRegressor = []\n",
    "    realAndPredictedValuesKNeighborsRegressor = []\n",
    "    realAndPredictedValuesDecisionTreeRegressor = []\n",
    "    realAndPredictedValuesLinearRegression = []\n",
    "    realAndPredictedValuesRANSACRegressor = []\n",
    "    \n",
    "    for index, row in outcomesDfTestAndCatCluster.iterrows():\n",
    "        #iteration through each row of the test dataset\n",
    "        \n",
    "        #real value of the column to be predicted\n",
    "        y_test_value = row[\"price\"]\n",
    "        #values given as input for the prediction model\n",
    "        x_test_value = row[[\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]].values\n",
    "        #product category cluster for this row of the dataset\n",
    "        clusterIndex = row[\"cat_cluster\"]\n",
    "\n",
    "        #the prediction models associated to this cluster are extracted\n",
    "        modelRandomForestRegressor = clusterIndexToRandomForestRegressor[clusterIndex]\n",
    "        modelKNeighborsRegressor = clusterIndexToKNeighborsRegressor[clusterIndex]\n",
    "        modelDecisionTreeRegressor = clusterIndexToDecisionTreeRegressor[clusterIndex]\n",
    "        modelLinearRegression = clusterIndexToLinearRegression[clusterIndex]\n",
    "        \n",
    "        realAndPredictedValuesRandomForestRegressor.append((y_test_value,modelRandomForestRegressor.predict([x_test_value])[0]))\n",
    "        realAndPredictedValuesKNeighborsRegressor.append((y_test_value,modelKNeighborsRegressor.predict([x_test_value])[0]))\n",
    "        realAndPredictedValuesDecisionTreeRegressor.append((y_test_value,modelDecisionTreeRegressor.predict([x_test_value])[0]))\n",
    "        realAndPredictedValuesLinearRegression.append((y_test_value,modelLinearRegression.predict([x_test_value])[0]))\n",
    "        \n",
    "        if clusterIndex not in clusterIndexToRANSACRegressor:\n",
    "            print(\"clusterIndex not in clusterIndexToRANSACRegressor\")\n",
    "            continue\n",
    "        modelRANSACRegressor = clusterIndexToRANSACRegressor[clusterIndex]\n",
    "        realAndPredictedValuesRANSACRegressor.append((y_test_value,modelRANSACRegressor.predict([x_test_value])[0]))\n",
    "    \n",
    "    #RandomForestRegressor\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesRandomForestRegressor)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRandomForestRegressor.append(kFoldMedianAbsoluteError) \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRandomForestRegressor.append(kFoldMeanAbsoluteError) \n",
    "    \n",
    "    #KNeighborsRegressor\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesKNeighborsRegressor)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsKNeighborsRegressor.append(kFoldMedianAbsoluteError) \n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsKNeighborsRegressor.append(kFoldMeanAbsoluteError) \n",
    "    \n",
    "    #DecisionTreeRegressor\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesDecisionTreeRegressor)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsDecisionTreeRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsDecisionTreeRegressor.append(kFoldMeanAbsoluteError) \n",
    "    \n",
    "    #LinearRegression\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesLinearRegression)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsLinearRegression.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsLinearRegression.append(kFoldMeanAbsoluteError) \n",
    "    \n",
    "    #RANSACRegressor\n",
    "    #all real and predicted values are extracted and the metrics are calculated\n",
    "    y_test, P_price = zip(*realAndPredictedValuesRANSACRegressor)    \n",
    "    kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "    kFoldMedianAbsoluteErrorsRANSACRegressor.append(kFoldMedianAbsoluteError)\n",
    "    kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "    kFoldMeanAbsoluteErrorsRANSACRegressor.append(kFoldMeanAbsoluteError) \n",
    "    \n",
    "medianAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRandomForestRegressor)\n",
    "meanAbsoluteErrorsRandomForestRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRandomForestRegressor)\n",
    "medianAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsKNeighborsRegressor)\n",
    "meanAbsoluteErrorsKNeighborsRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsKNeighborsRegressor)\n",
    "medianAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsDecisionTreeRegressor)\n",
    "meanAbsoluteErrorsDecisionTreeRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsDecisionTreeRegressor)\n",
    "medianAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMedianAbsoluteErrorsLinearRegression)\n",
    "meanAbsoluteErrorsLinearRegressionAverage = np.mean(kFoldMeanAbsoluteErrorsLinearRegression)\n",
    "medianAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMedianAbsoluteErrorsRANSACRegressor)\n",
    "meanAbsoluteErrorsRANSACRegressorAverage = np.mean(kFoldMeanAbsoluteErrorsRANSACRegressor)\n",
    "\n",
    "print(\"--\")\n",
    "print(\"Random Forest Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRandomForestRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"K Neighbors Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsKNeighborsRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Decision Tree Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsDecisionTreeRegressorAverage))\n",
    "print(\"--\")\n",
    "print(\"Linear Regression\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsLinearRegressionAverage))\n",
    "print(\"--\")\n",
    "print(\"RANSAC Regressor\")\n",
    "print(\"Median absolute error: \"+str(medianAbsoluteErrorsRANSACRegressorAverage))\n",
    "print(\"Mean absolute error: \"+str(meanAbsoluteErrorsRANSACRegressorAverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen above, the model with the best results in terms of the median absolute error is the RANSAC regressor, while the model with the best results in terms of the mean absolute error is the random forest regressor. The results for the decision tree regressor are very similar to the ones obtained with the random forest regressor. The k-neighbors regressor results are, although worse, very similar too. The worst results are obtained with the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the results obtained with this set of models are worse than the the results obtained with the set of models in other sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of effort, this model requires several preprocessing steps: given the product description of an item in the column \"desc\", its Amazon category must be obtained, and afterwards, the corresponding word embedding vector. Moreover, the retail price of the item has to be scaled before making the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appart from the preprocessing steps, during the training part of the model, the clusters and the scaler instance are obtained, and they are later used everytime that a new prediction is made.  After some time, more and more new products will begin to be auctioned, which will not have been considered to form the clusters and to calculate the scaling values.  Because of this, the clusters and the scaler instance may eventually become outdated, and the models will perform worse. Therefore, appart from the preprocessing step, it is necessary to retrain the prediction models from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As compared with the previous section in which a single model is used, the results obtained with multiple models in this section are better, but when retraining is necessary, multipled models have to be retrained instead of just one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the given dataset, this set of models is not interesting, since it is more complex and performs worse than some of the set of models explained in other sections. As the amount of data increases, it could be interesting to analyze the performance of this set of models again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing and optimizing the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the different models have been analyzed in the above sections. An important remark is that the chosen values for the different variables used in each section have an impact on the results. These variables include: the input parameters of the prediction model algorithms, the number of folds used for the k-fold cross-validation, the number of clusters given as input for the K-Means algorithm for the models that depend on clustering of the data, the input parameters for the scaler algorithm for the models that depend on scaling the input data, etc. \n",
    "\n",
    "All of these variables should ideally be opmitized according to the metric that defines the quality of the prediction model (for example, using the GridSearchCV function available in the sklearn library). This requires a lot of time, since there are a lot of possible combinations for all parameter values. For the input parameters of the prediction models, the default parameter values have been used in the results calculated above, except for the random state that has been set to the same value everytime. The number of clusters for the the K-Means algorithm has been manually modified to find the number of clusters that optimizes the result for the values that have been tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the random forest regressor and the decision tree regressor are the algorithms that normally provide the best results. These two models are relatively robust to outliers, since they isolate atypical observations into small leaves. The random forest regressor generally performs a bit better than the decision tree regressor and is less likely to overfit the data. The disadvantage is that random forest regressor are more difficult to interpret than decision tree regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different clustering options have been analyzed: one that uses the word embedding vectors to cluster the items according to their product categories, and another one that, appart from the word embedding vectors, it also uses the retail prices of the items to obtain the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the available data, the model that obtains the clusters by only using as input the word embedding vectors performs better. Moreover, it also requires less preprocessing steps and having to retrain the model less often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results obtained with the models that take into account the product categories are better than the ones obtained with the models that do not take them into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, taking into account the product categories requires a bigger effort, since a preprocessing step is necessary to make a new prediction: given the product description of an item in the column \"desc\", its Amazon category must be obtained, and afterwards, the corresponding word embedding vector.\n",
    "\n",
    "Furthermore, the clusters product category clusters obtained during the trianing part of the model are the ones that are used each time that a new prediction is made. After some time, more and more new products will begin to be auctioned and the clusters may eventually become outdated, causing the model to perform worse. Therefore, appart from the preprocessing step, using the product categories also involves having to retrain the prediction model from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the models that take only take into account the word embedding vectors to form the clusters, two different versions have been analyzed: one consisting on a single prediction model and another one that consists on a set of different models, where the model chosen to make every prediction depends on the cluster that each row in the test data belongs to.\n",
    "\n",
    "The results obtained with the set of multiple models were slightly better than the ones obtained when using a single model. Nevertheless, one of the prediction models corresponding to a certain cluster might perform much worse than the others. \n",
    "\n",
    "Also, in both versions, the clusters will eventually become outdated, and when a retraining is necessary, a single prediction model will have to be retrained in one of the versions, while in the other one, as many prediction models as existing clusters will have to be retrained. Considering this disadvantage, as well as the bigger complexity that using a set of multiple models implies, and the fact that the results are fairly similar for both versions, it has been decided to choose the version consisting on a single prediction model as the final one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the final chosen set of models is the one explained in section \"b) One Model - Having into account the categories\". Of the different prediction algorithms used in that section, the one with the best results is the random forest regressor. This is the one that has been implemented as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables that have a clear impact on the results for this model are: the number of clusters chosen for the K-Means clustering algorithm, as well as the input parameters for the random forest regressor. A grid of different values for the number of clusters and the number of trees used in the random forest algorithm has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomesDf = pd.read_csv('./outcomes_clean.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDescriptionToVectorDf = pd.read_excel('./productDescriptionToVector.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing grid combination 1 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 2 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 3 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 4 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 5 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 6 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 7 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 8 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 9 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 10 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 11 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 12 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 13 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 14 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 15 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 16 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 17 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 18 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 19 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 20 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 21 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 22 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 23 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 24 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 25 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 26 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 27 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 28 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 29 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n",
      "Executing grid combination 30 out of 30 combinations\n",
      "Executing k fold = 1\n",
      "Executing k fold = 2\n",
      "Executing k fold = 3\n",
      "Executing k fold = 4\n",
      "Executing k fold = 5\n"
     ]
    }
   ],
   "source": [
    "kFoldMedianAbsoluteErrorsRandomForestRegressor = {}\n",
    "kFoldMeanAbsoluteErrorsRandomForestRegressor = {}\n",
    "\n",
    "nClusterGrid = [10,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "#nClusterGrid = [15]\n",
    "nEstimatorsGrid=[10,15]\n",
    "\n",
    "gridTotalCombinations = len(nClusterGrid)*len(nEstimatorsGrid)\n",
    "gridCombinationNumber = 1\n",
    "\n",
    "for nClustersGridValue in nClusterGrid:\n",
    "    for nEstimatorsGridValue in nEstimatorsGrid:\n",
    "        gridCombinationString = str(nClustersGridValue)+\"-\"+str(nEstimatorsGridValue)\n",
    "        print(\"Executing grid combination \"+str(gridCombinationNumber)+\" out of \"+str(gridTotalCombinations)+\" combinations\")\n",
    "        gridCombinationNumber=gridCombinationNumber+1\n",
    "\n",
    "        kFoldNumber = 1\n",
    "        kf = KFold(n_splits=5, random_state=1)\n",
    "        for train_index, test_index in kf.split(outcomesDf):\n",
    "            #5-fold cross-validation\n",
    "            print(\"Executing k fold = \"+str(kFoldNumber))\n",
    "            kFoldNumber=kFoldNumber+1\n",
    "\n",
    "            #train and test split\n",
    "            outcomesDf_train, outcomesDf_test = outcomesDf.iloc[train_index], outcomesDf.iloc[test_index]   \n",
    "\n",
    "            #TRAINING PART\n",
    "\n",
    "            #Unique product descriptions present in the training dataset\n",
    "            productDescriptionTrain = outcomesDf_train['desc'].unique()\n",
    "\n",
    "            productDescriptionToVector={}\n",
    "            for item in productDescriptionTrain:\n",
    "                #Word2Vec vector obtained for each product description\n",
    "                productDescriptionToVector[item] = productDescriptionToVectorDf[item]\n",
    "\n",
    "            productVectors = list(productDescriptionToVector.values())\n",
    "\n",
    "            km = KMeans(n_clusters=nClustersGridValue,random_state=2)\n",
    "            #the clustering is performed with the Word2Vec vectors for the product categories given as input\n",
    "            km.fit(productVectors)\n",
    "            clusters = km.labels_.tolist()\n",
    "            #DataFrame containing columns for the product description and the category cluster that it is associated to\n",
    "            clusteredCategoriesDf = pd.DataFrame({'desc': list(productDescriptionToVector.keys()), 'cat_cluster': clusters})\n",
    "            #One-Hot encoded version of the DataFrame containing a column for each cluster\n",
    "            clusteredCategoriesDfOneHotEnc = convertCategoryDataFrameToOneHotEncodedVersion(clusteredCategoriesDf,'cat_cluster')  \n",
    "            #column names of the clusters\n",
    "            cluster_column_names = clusteredCategoriesDfOneHotEnc.columns.values\n",
    "            clusteredCategoriesDfOneHotEnc['desc'] = clusteredCategoriesDf['desc']\n",
    "            #the training dataset is merged so that it contains a one hot-encoded \n",
    "            #column for each one of the clusters obtained.\n",
    "            outcomesDfWithCatOneHot = outcomesDf_train.merge(clusteredCategoriesDfOneHotEnc,how='left')\n",
    "            normal_x_column_names = [\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]\n",
    "            #column names to be used in the prediction model: they include the columns corresponding to the clusters obtained.\n",
    "            column_names_with_clusters = np.concatenate([normal_x_column_names,cluster_column_names])\n",
    "\n",
    "            #training phase variables\n",
    "            X_train = outcomesDfWithCatOneHot[column_names_with_clusters]\n",
    "            y_train = outcomesDfWithCatOneHot[\"price\"]\n",
    "\n",
    "            #TEST PART\n",
    "\n",
    "            #Unique product descriptions present in the training dataset\n",
    "            productDescriptionTest = outcomesDf_test['desc'].unique()\n",
    "\n",
    "            productDescriptionToVector={}\n",
    "            for item in productDescriptionTest:\n",
    "                #Word2Vec vector obtained for each product description\n",
    "                productDescriptionToVector[item] = productDescriptionToVectorDf[item]\n",
    "\n",
    "            productDescriptionToTrainCluster = {}\n",
    "            for productDescription, vector in productDescriptionToVector.items():\n",
    "                #The K-Means model obtained during the training phase is used to associate\n",
    "                #an existing cluster to each one of the product vectors contained in the test data.\n",
    "                clusterCategory = km.predict([vector])\n",
    "                productDescriptionToTrainCluster[productDescription] = clusterCategory[0]\n",
    "\n",
    "            #DataFrame containing columns for the product description and the category cluster that it is associated to\n",
    "            clusteredCategoriesDf = pd.DataFrame(list(productDescriptionToTrainCluster.items()), columns=['desc', 'cat_cluster'])\n",
    "            #One-Hot encoded version of the DataFrame containing a column for each cluster.\n",
    "            #The columns are the same ones as in the training phase\n",
    "            clusteredCategoriesDfOneHotEnc = convertCategoryDataFrameToOneHotEncodedVersion(clusteredCategoriesDf,'cat_cluster',cluster_column_names)\n",
    "            #column names of the clusters\n",
    "            cluster_column_names = clusteredCategoriesDfOneHotEnc.columns.values\n",
    "            clusteredCategoriesDfOneHotEnc['desc'] = clusteredCategoriesDf['desc']\n",
    "            #the test dataset is merged so that it contains a one hot-encoded \n",
    "            #column for each one of the clusters obtained.\n",
    "            outcomesDfWithCatOneHot = outcomesDf_test.merge(clusteredCategoriesDfOneHotEnc,how='left')\n",
    "            normal_x_column_names = [\"retail\",\"bidincrement\",\"bidfee\",\"flg_click_only\",\"flg_beginnerauction\",\"flg_fixedprice\",\"flg_endprice\"]\n",
    "            #column names to be used in the prediction model: they include the columns corresponding to the clusters obtained.\n",
    "            column_names_with_clusters = np.concatenate([normal_x_column_names,cluster_column_names])\n",
    "\n",
    "            #test phase variables\n",
    "            X_test = outcomesDfWithCatOneHot[column_names_with_clusters]\n",
    "            y_test = outcomesDfWithCatOneHot[\"price\"]\n",
    "\n",
    "            #Predictors\n",
    "\n",
    "            #RandomForestRegressor\n",
    "            model=RandomForestRegressor(random_state=1, n_estimators=nEstimatorsGridValue)\n",
    "            model.fit(X_train,y_train)\n",
    "            P_price = model.predict(X_test)\n",
    "\n",
    "            kFoldMedianAbsoluteError = median_absolute_error(y_test,P_price)\n",
    "            if gridCombinationString not in kFoldMedianAbsoluteErrorsRandomForestRegressor:\n",
    "                kFoldMedianAbsoluteErrorsRandomForestRegressor[gridCombinationString] = []\n",
    "            kFoldMedianAbsoluteErrorsRandomForestRegressor[gridCombinationString].append(kFoldMedianAbsoluteError)               \n",
    "            \n",
    "            kFoldMeanAbsoluteError = mean_absolute_error(y_test,P_price)\n",
    "            if gridCombinationString not in kFoldMeanAbsoluteErrorsRandomForestRegressor:\n",
    "                kFoldMeanAbsoluteErrorsRandomForestRegressor[gridCombinationString] = []\n",
    "            kFoldMeanAbsoluteErrorsRandomForestRegressor[gridCombinationString].append(kFoldMeanAbsoluteError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N clusters = 10 ; N estimators = 10\n",
      "Median absolute error = 10.4600742337\n",
      "Mean absolute error   = 28.3928103559\n",
      "--\n",
      "N clusters = 10 ; N estimators = 15\n",
      "Median absolute error = 10.4541755616\n",
      "Mean absolute error   = 28.3287636468\n",
      "--\n",
      "N clusters = 12 ; N estimators = 10\n",
      "Median absolute error = 10.4121693664\n",
      "Mean absolute error   = 28.3481462479\n",
      "--\n",
      "N clusters = 12 ; N estimators = 15\n",
      "Median absolute error = 10.4060284261\n",
      "Mean absolute error   = 28.2630361398\n",
      "--\n",
      "N clusters = 13 ; N estimators = 10\n",
      "Median absolute error = 10.5492960772\n",
      "Mean absolute error   = 28.3659670918\n",
      "--\n",
      "N clusters = 13 ; N estimators = 15\n",
      "Median absolute error = 10.5536345395\n",
      "Mean absolute error   = 28.2897791786\n",
      "--\n",
      "N clusters = 14 ; N estimators = 10\n",
      "Median absolute error = 10.4384989764\n",
      "Mean absolute error   = 28.3397031232\n",
      "--\n",
      "N clusters = 14 ; N estimators = 15\n",
      "Median absolute error = 10.457491468\n",
      "Mean absolute error   = 28.2555242302\n",
      "--\n",
      "N clusters = 15 ; N estimators = 10\n",
      "Median absolute error = 10.5067559321\n",
      "Mean absolute error   = 28.6887268744\n",
      "--\n",
      "N clusters = 15 ; N estimators = 15\n",
      "Median absolute error = 10.5239521751\n",
      "Mean absolute error   = 28.6054910342\n",
      "--\n",
      "N clusters = 16 ; N estimators = 10\n",
      "Median absolute error = 10.4286437528\n",
      "Mean absolute error   = 28.2995682388\n",
      "--\n",
      "N clusters = 16 ; N estimators = 15\n",
      "Median absolute error = 10.4295892271\n",
      "Mean absolute error   = 28.2082201044\n",
      "--\n",
      "N clusters = 17 ; N estimators = 10\n",
      "Median absolute error = 10.4505840588\n",
      "Mean absolute error   = 28.3585533857\n",
      "--\n",
      "N clusters = 17 ; N estimators = 15\n",
      "Median absolute error = 10.4818799252\n",
      "Mean absolute error   = 28.3006732782\n",
      "--\n",
      "N clusters = 18 ; N estimators = 10\n",
      "Median absolute error = 10.3837651768\n",
      "Mean absolute error   = 28.3439120603\n",
      "--\n",
      "N clusters = 18 ; N estimators = 15\n",
      "Median absolute error = 10.3922842474\n",
      "Mean absolute error   = 28.2737346206\n",
      "--\n",
      "N clusters = 19 ; N estimators = 10\n",
      "Median absolute error = 10.5835055952\n",
      "Mean absolute error   = 28.5438549368\n",
      "--\n",
      "N clusters = 19 ; N estimators = 15\n",
      "Median absolute error = 10.5371342501\n",
      "Mean absolute error   = 28.4718779571\n",
      "--\n",
      "N clusters = 20 ; N estimators = 10\n",
      "Median absolute error = 10.3695943096\n",
      "Mean absolute error   = 28.2917930555\n",
      "--\n",
      "N clusters = 20 ; N estimators = 15\n",
      "Median absolute error = 10.3774828774\n",
      "Mean absolute error   = 28.2364238654\n",
      "--\n",
      "N clusters = 21 ; N estimators = 10\n",
      "Median absolute error = 10.923319363\n",
      "Mean absolute error   = 29.9601147561\n",
      "--\n",
      "N clusters = 21 ; N estimators = 15\n",
      "Median absolute error = 10.9759501202\n",
      "Mean absolute error   = 29.9218892957\n",
      "--\n",
      "N clusters = 22 ; N estimators = 10\n",
      "Median absolute error = 10.6002101174\n",
      "Mean absolute error   = 28.9395753549\n",
      "--\n",
      "N clusters = 22 ; N estimators = 15\n",
      "Median absolute error = 10.6107018596\n",
      "Mean absolute error   = 28.891882566\n",
      "--\n",
      "N clusters = 23 ; N estimators = 10\n",
      "Median absolute error = 10.9381481613\n",
      "Mean absolute error   = 30.2791487335\n",
      "--\n",
      "N clusters = 23 ; N estimators = 15\n",
      "Median absolute error = 10.9291463893\n",
      "Mean absolute error   = 30.2056939018\n",
      "--\n",
      "N clusters = 24 ; N estimators = 10\n",
      "Median absolute error = 10.3951699895\n",
      "Mean absolute error   = 28.3200529226\n",
      "--\n",
      "N clusters = 24 ; N estimators = 15\n",
      "Median absolute error = 10.4100309657\n",
      "Mean absolute error   = 28.2503207347\n",
      "--\n",
      "N clusters = 25 ; N estimators = 10\n",
      "Median absolute error = 10.4371148382\n",
      "Mean absolute error   = 28.4703631012\n",
      "--\n",
      "N clusters = 25 ; N estimators = 15\n",
      "Median absolute error = 10.4294678722\n",
      "Mean absolute error   = 28.3555403948\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "minMedianAbsoluteError = None\n",
    "minMedianAbsoluteErrorKey = None\n",
    "meanAbsoluteErrorWithMinMedianAbsoluteError = None\n",
    "\n",
    "\n",
    "minMeanAbsoluteError= None\n",
    "minMeanAbsoluteErrorKey = None\n",
    "medianAbsoluteErrorWithMinMeanAbsoluteError = None\n",
    "\n",
    "\n",
    "for key, value in kFoldMedianAbsoluteErrorsRandomForestRegressor.items():\n",
    "    nClustersGridValue = key[0:2]\n",
    "    nEstimatorsGridValue = key[-2:]\n",
    "    print(\"N clusters = \" + str(nClustersGridValue) + \" ; N estimators = \"+str(nEstimatorsGridValue))\n",
    "    \n",
    "    kFoldMedianAbsoluteErrorsRandomForestRegressorCurrentCombination = kFoldMedianAbsoluteErrorsRandomForestRegressor[key]\n",
    "    kFoldMeanAbsoluteErrorsRandomForestRegressorCurrentCombination = kFoldMeanAbsoluteErrorsRandomForestRegressor[key]    \n",
    "    \n",
    "    combinationMedianAbsoluteError = np.mean(kFoldMedianAbsoluteErrorsRandomForestRegressorCurrentCombination)\n",
    "    combinationMeanAbsoluteError = np.mean(kFoldMeanAbsoluteErrorsRandomForestRegressorCurrentCombination)\n",
    "    \n",
    "    if minMedianAbsoluteError is None:\n",
    "        minMedianAbsoluteError = combinationMedianAbsoluteError\n",
    "        minMedianAbsoluteErrorKey = key\n",
    "        meanAbsoluteErrorWithMinMedianAbsoluteError = combinationMeanAbsoluteError       \n",
    "    elif combinationMedianAbsoluteError < minMedianAbsoluteError:\n",
    "        minMedianAbsoluteError = combinationMedianAbsoluteError\n",
    "        minMedianAbsoluteErrorKey = key     \n",
    "        meanAbsoluteErrorWithMinMedianAbsoluteError = combinationMeanAbsoluteError\n",
    "            \n",
    "    if minMeanAbsoluteError is None:\n",
    "        minMeanAbsoluteError = combinationMeanAbsoluteError\n",
    "        minMeanAbsoluteErrorKey = key\n",
    "        medianAbsoluteErrorWithMinMeanAbsoluteError = combinationMedianAbsoluteError\n",
    "    elif combinationMeanAbsoluteError < minMeanAbsoluteError:\n",
    "        minMeanAbsoluteError = combinationMeanAbsoluteError\n",
    "        minMeanAbsoluteErrorKey = key\n",
    "        medianAbsoluteErrorWithMinMeanAbsoluteError = combinationMedianAbsoluteError\n",
    "    \n",
    "    print(\"Median absolute error = \" + str(combinationMedianAbsoluteError))\n",
    "    print(\"Mean absolute error   = \" + str(combinationMeanAbsoluteError))\n",
    "      \n",
    "    print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal median absolute error is 10.3695943096 with 20 clusters and 10 estimators.\n",
      "With this combination the mean absolute error is 28.2917930555\n",
      "---\n",
      "The optimal median absolute error is 28.2082201044 with 16 clusters and 15 estimators.\n",
      "With this combination the median absolute error is 10.4295892271\n"
     ]
    }
   ],
   "source": [
    "minMedianAbsoluteErrorNClustersGridValue = minMedianAbsoluteErrorKey[0:2]\n",
    "minMedianAbsoluteErrorNEstimatorsGridValue = minMedianAbsoluteErrorKey[-2:]\n",
    "\n",
    "print('The optimal median absolute error is ' + str(minMedianAbsoluteError) + \" with \" + str(minMedianAbsoluteErrorNClustersGridValue) + \" clusters and \" + str(minMedianAbsoluteErrorNEstimatorsGridValue) + \" estimators.\")\n",
    "print('With this combination the mean absolute error is '+str(meanAbsoluteErrorWithMinMedianAbsoluteError))\n",
    "minMeanAbsoluteErrorNClustersGridValue = minMeanAbsoluteErrorKey[0:2]\n",
    "minMeanAbsoluteErrorNEstimatorsGridValue = minMeanAbsoluteErrorKey[-2:]\n",
    "print(\"---\")\n",
    "print('The optimal median absolute error is ' + str(minMeanAbsoluteError) +\" with \" + str(minMeanAbsoluteErrorNClustersGridValue) +\" clusters and \" + str(minMeanAbsoluteErrorNEstimatorsGridValue) + \" estimators.\")\n",
    "print('With this combination the median absolute error is '+str(medianAbsoluteErrorWithMinMeanAbsoluteError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it has been mentioned at the beginning of this document when the metrics have been introduced, the median absolute error is considered to be more important for auction selling price predictions. Therefore, the optimal parameters and the final metric results are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen set of models : b) One Model - Having into account the categories\n",
      "Chosen model : Random forest regressor\n",
      "K-Means -> N clusters = 20\n",
      "Random forest regressor -> N estimators  = 10\n",
      "Median absolute error = 10.3695943096\n",
      "Mean absolute error = 28.2917930555\n"
     ]
    }
   ],
   "source": [
    "print(\"Chosen set of models : \"+\"b) One Model - Having into account the categories\")\n",
    "print(\"Chosen model : \"+\"Random forest regressor\")\n",
    "print(\"K-Means -> N clusters = \"+str(minMedianAbsoluteErrorNClustersGridValue))\n",
    "print(\"Random forest regressor -> N estimators  = \"+str(minMedianAbsoluteErrorNEstimatorsGridValue))\n",
    "print(\"Median absolute error = \"+str( minMedianAbsoluteError))\n",
    "print(\"Mean absolute error = \"+str(meanAbsoluteErrorWithMinMedianAbsoluteError))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
